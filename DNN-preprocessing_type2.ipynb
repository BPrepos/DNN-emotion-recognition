{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6by9Hq0pz9AM"
      },
      "source": [
        "from numpy import loadtxt, array\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9YJSmUVR_Tf",
        "outputId": "2d638f85-d650-4a54-bf7e-91009464199a"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R23Bet1Qmf8S"
      },
      "source": [
        "files = []\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s01.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s02.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s03.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s04.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s05.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s06.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s07.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s08.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s09.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s10.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s11.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s12.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s13.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s14.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s15.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s16.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s17.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s18.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s19.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s20.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s21.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s22.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s23.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s24.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s25.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s26.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s27.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s28.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s29.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s30.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s31.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s32.dat')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3CmentroEDU"
      },
      "source": [
        "# wczytanie danych i preprocessing 2\n",
        "table_of_dataX = []\n",
        "table_of_dataY = []\n",
        "for i in range(32):\n",
        "  f = files[i]\n",
        "  with open(f, 'rb') as f: content = pickle.load(f, encoding='latin1')\n",
        "  data = content['data']\n",
        "  labels = content['labels']\n",
        "  for j in range(40): #40 filmow\n",
        "    tmpX = data[j]\n",
        "    tmpY = labels[j]\n",
        "    for l in range(7):\n",
        "      table_of_dataY.append(tmpY)\n",
        "      signal_number = l*1000 + 1000\n",
        "      vector_input = []\n",
        "      for k in range(40):\n",
        "        mean = np.mean(tmpX[k][signal_number:signal_number+1000])\n",
        "        median = np.median(tmpX[k][signal_number:signal_number+1000])\n",
        "        max = np.max(tmpX[k][signal_number:signal_number+1000])\n",
        "        min = np.min(tmpX[k][signal_number:signal_number+1000])\n",
        "        variance = np.var(tmpX[k][signal_number:signal_number+1000])\n",
        "        skewness = skew(tmpX[k][signal_number:signal_number+1000])\n",
        "        kur = kurtosis(tmpX[k][signal_number:signal_number+1000])\n",
        "        vector_input.append(mean)\n",
        "        vector_input.append(median)\n",
        "        vector_input.append(max)\n",
        "        vector_input.append(min)\n",
        "        vector_input.append(variance)\n",
        "        vector_input.append(skewness)\n",
        "        vector_input.append(kur)\n",
        "      table_of_dataX.append(vector_input)\n",
        "\n",
        "\n",
        "        \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4I9C_wPtTyj"
      },
      "source": [
        "table_of_dataX = np.asarray(table_of_dataX)\n",
        "table_of_dataY = np.asarray(table_of_dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlBZVZigH0eu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHzPV3vpnlSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8d9a89-941d-4893-c503-ca76e556fb37"
      },
      "source": [
        "print(len(table_of_dataX))\n",
        "print(len(table_of_dataY))\n",
        "randnums = [1384, 5436, 5904, 6490,   76, 2066, 5068, 6085, 4360, 2995, 4069,\n",
        "       8334, 6052, 6078, 1161, 1434,  123, 5356, 6270, 3977, 1882, 2556,\n",
        "       6286, 5572, 6719, 2112, 5237, 4023, 1925, 3396, 5688, 2261, 1458,\n",
        "       3869, 7851, 6049, 2790, 6500, 3084, 4834, 8263,  774, 6964, 3561,\n",
        "        254, 2713, 6468, 8799, 2394, 6291, 5306, 5358, 7856, 3734,  928,\n",
        "       6360, 7380, 8526,   54, 7409, 2318, 4415, 5057, 4387, 8366, 2255,\n",
        "         16, 3335, 7950, 5598, 8599, 5785,  342, 8214, 8417, 6012, 2856,\n",
        "       6481, 2403, 7391, 4168, 7017, 2347,  525, 6317, 7094, 3157, 6739,\n",
        "       8934, 4432, 6540, 6918,  655, 6005, 5727, 8691, 7599, 4671, 3492,\n",
        "       2632, 3687, 2327, 4030, 2580, 4781, 7005, 5494,  668, 4913,  919,\n",
        "       2385, 6939, 1746, 4729, 6752, 1895, 5131, 5024, 3809, 6461, 1976,\n",
        "       8156, 3463, 3271, 5393, 7203, 8580,  863, 1632, 3217, 2501, 2633,\n",
        "        691, 3954,  647, 7216, 4252, 2172, 3248, 5536, 8937, 2378, 1874,\n",
        "       3046, 2573, 5266, 3426,  992, 2147, 2314, 4304, 5168, 1611, 3998,\n",
        "       4018, 4580, 2538, 5037, 1915, 5414, 1708, 4221, 7267,  628, 1771,\n",
        "        487, 3290,   75, 1673, 2636, 7300,  169, 4796, 6829, 6357, 5034,\n",
        "       6025,  930, 7758, 6761, 3302, 2564, 2915,  625,   28, 5147, 4678,\n",
        "       4849,  384, 6197, 5159, 6491, 4155, 7387, 7742,  190,  155, 7302,\n",
        "       5490, 8688, 2226, 4378, 7898,  255, 8493, 6002, 2640, 1132, 4298,\n",
        "       3397, 2105,  507,  824, 5343, 7288, 8573,  425, 4885, 7767, 3446,\n",
        "       4709, 4504, 2192, 3962, 3526, 1705, 7159, 5132, 5510, 5791,  423,\n",
        "       1164, 5879,   97, 8729, 4535, 4703, 2051, 2769, 8062,  117, 1340,\n",
        "       3948, 4784,  269, 3909, 5736, 5613, 5756, 5950, 6749, 7268, 8893,\n",
        "       7848, 8152, 4951, 5959, 7385, 5245, 2510, 7979, 5404, 2033, 8460,\n",
        "       8666, 3731, 2745, 3647, 3287,  113, 8553, 8343,   30, 7762, 6321,\n",
        "       8083, 6894, 2832, 6495, 4364, 3807,  516, 3618, 5115, 8483, 6073,\n",
        "       7867,   20, 3692, 2968, 7920, 7708, 7405, 8888, 3898, 6108, 1468,\n",
        "       6043, 8679, 5182, 2127, 6203, 8102,  552, 2067, 4398, 6842, 2526,\n",
        "       5183, 3292, 7390, 5628, 8139, 3187, 7766, 2016, 2402, 2469,  226,\n",
        "       4965, 6559, 8420, 6863, 5728, 3466, 2541, 3899,  358, 3961,  331,\n",
        "       6574, 4016, 7083, 1082, 2550, 7228, 3027, 1263, 3790, 8283, 6834,\n",
        "       7981, 2753, 3310, 6457, 4776, 5575, 4076, 4568, 2487, 7059, 7065,\n",
        "       3014, 5504, 7813, 4952, 3401, 3728, 5009, 2803, 2094, 7080, 4979,\n",
        "       8584, 2211, 2405, 8665, 2119, 1916, 4928, 5742, 4802, 1239, 6482,\n",
        "       4503,  445, 5841,  979, 6783, 7791, 8316, 2032, 5953, 6621, 6543,\n",
        "       2741, 8349, 1586, 1709, 8434, 3861, 5425, 3078, 7902,  150, 6312,\n",
        "       3529, 6100, 4311, 1012,  114, 5275,  709,  237, 7585, 6423, 2730,\n",
        "       8110, 1805, 8262, 1292,  230, 8026, 5178, 2729, 1947, 5829, 2213,\n",
        "       5107, 8547, 2162, 6873, 3645, 3806, 4987, 7883, 3327, 2092, 8113,\n",
        "        970, 7675, 4444, 6023, 8383, 3877,  260, 1145,  197, 7580, 8958,\n",
        "       8217, 1844, 5270, 7538, 1537, 3438, 1668, 1546, 1053, 8127, 5284,\n",
        "       6265, 4607, 3969, 5189, 5983, 3770, 1619, 1604, 7489, 3578, 3069,\n",
        "       1652, 3648, 7117,  145, 8855, 8823, 5715, 7231, 1583, 2534, 2604,\n",
        "       5065, 1525, 4534, 5172, 2698, 8690, 3258,  450, 3740, 1186, 8296,\n",
        "       8467, 3193, 4902, 6903, 7384, 3574, 8662, 1970, 5503,  752, 6145,\n",
        "       6388, 5537, 8211, 3348, 2999, 8908, 6283, 6187, 5901, 2905, 3135,\n",
        "       6213, 4775, 3030, 8187,  545, 8765, 5695, 8453, 1811, 5552, 2278,\n",
        "       1268,  821, 3864, 3002,  903, 2784, 5400, 4279, 7386, 7726, 6768,\n",
        "       4828, 4110, 4427, 3478, 3400, 1077, 5970, 5941, 7421, 8535, 4512,\n",
        "       1351, 2475, 4414, 5875,  246, 7165, 6603, 6170, 2717, 8704, 8112,\n",
        "       3718, 2449,  491, 5502, 3927, 4598,  400, 7531, 8025, 6544, 1756,\n",
        "       2584, 1435, 3839, 7221, 7839, 3841,  889, 2476, 7976, 4942, 7816,\n",
        "       7881, 8288, 1450, 3391, 8633, 5748, 4050, 4319, 4777, 5474, 5489,\n",
        "       8606, 1023, 2069, 2782, 5822, 5421,  806, 2085, 2083, 3374, 2149,\n",
        "       5586, 3614, 4954, 7025, 4048, 7567, 1316, 4012, 4895, 8093, 7395,\n",
        "       2840, 8181, 6199, 7455, 7927, 6076, 1501, 8046,  303, 4997, 3065,\n",
        "       1629, 7800, 8131, 4674, 7621,  939, 1591, 5579,  498,  119, 4290,\n",
        "       5328, 6191, 2269, 2533,  317,  773, 1802, 7733, 8274, 5040, 1189,\n",
        "        472, 8596, 5923, 4661, 4718, 1955, 5863, 7542, 3564, 8374, 7480,\n",
        "       5750, 4247, 5680,  921,  382, 1639, 1710, 4604,  393, 5694, 1312,\n",
        "       5063, 3500, 7179, 3493, 7081, 4797, 7510,  908, 6212, 3088, 1330,\n",
        "       6536, 1819, 4371, 7048, 4814, 2177, 1456, 3933, 4227, 1803, 8386,\n",
        "       8240, 4163,  918, 4267, 1007, 1769, 7549, 1313, 2462, 7962, 8155,\n",
        "       1893, 5173, 4600, 7878, 7256, 6888, 8272, 2976, 7609, 2148, 4741,\n",
        "       6344, 5096, 1631, 5216, 8647, 8699, 7258, 1890, 4217, 8475, 4348,\n",
        "       7553,   59, 2684,  288, 2257, 7995, 7183, 8505, 7474, 7400, 2049,\n",
        "       1020, 8175, 5327, 7397,  298,  378,  129, 1355, 1550,  394,  690,\n",
        "       1834, 6047, 2110,  902, 7134, 2718, 3086, 7124, 2972,  883,  714,\n",
        "       7148, 5280, 4548, 2540, 4379, 5348, 7581, 6913, 6000,  994, 5413,\n",
        "       5582, 7956, 8946, 4625, 3129,  601, 2099, 7020, 3214, 2188, 4169,\n",
        "       1360, 8718, 1353, 4164, 4602, 7652, 1033,  229, 2562, 2971, 7121,\n",
        "       2965, 3782, 6833, 3896, 7445, 5346, 1256, 1529, 1179, 3314, 5021,\n",
        "        131, 8295, 1255, 8901, 4208, 6109, 7135, 5305, 8027, 2876, 6184,\n",
        "       4394, 7332, 5522, 4582, 6667, 3060, 1930, 2699,  105, 5538, 1982,\n",
        "       1460,  837, 5482, 8836, 7342, 6036,  572,  999, 6809,  193, 4140,\n",
        "       8607, 6275,  372, 6989,  730, 7360, 1038, 2158, 6685, 6643, 5082,\n",
        "       8670, 6831, 8401, 2543, 3498, 6725, 3608, 7415, 5925, 6451, 3481,\n",
        "       5250, 3383, 5767,  208, 5796, 3266, 2825, 3720,  380, 3262, 3679,\n",
        "       6564, 1412, 5997, 6753, 8425, 4565, 4791, 3652,  589, 1806, 4291,\n",
        "       4060, 7871, 4649, 1610, 6923, 6675, 7032, 4748,  688, 4795,  236,\n",
        "       4136, 8369, 2060, 3646, 6008, 7077, 6048, 1199, 5787, 6206, 8237,\n",
        "       6554, 8733, 4266, 5815, 1119]\n",
        "testdataX = []\n",
        "testdataY = []\n",
        "for i in randnums:\n",
        "  testdataX.append(table_of_dataX[i])\n",
        "  testdataY.append(table_of_dataY[i])\n",
        "table_of_dataX_learn = np.delete(table_of_dataX, randnums, 0)\n",
        "table_of_dataY_learn = np.delete(table_of_dataY, randnums, 0)\n",
        "print(len(table_of_dataX_learn))\n",
        "print(len(table_of_dataY_learn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8960\n",
            "8960\n",
            "8064\n",
            "8064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASPEqxuzrohL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2390ee3c-0736-49c6-b769-5a46760452a2"
      },
      "source": [
        "divider_array = np.max(np.abs(table_of_dataX_learn), axis=0)\n",
        "table_of_dataX_learn_normed = table_of_dataX_learn/divider_array\n",
        "testdataX_normed = testdataX/divider_array\n",
        "testdataY = np.asarray(testdataY)\n",
        "len(table_of_dataX_learn_normed)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8064"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxukLCdBFaw4"
      },
      "source": [
        "\n",
        "class MyCustomCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        res_eval_1 = self.model.evaluate(testdataX_normed, testdataY, verbose = 0)\n",
        "        print(res_eval_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEYFxnk1OJBR",
        "outputId": "751e2fc3-0176-419a-e876-93413cc0a84e"
      },
      "source": [
        "num_folds = 5\n",
        "fold_no = 1\n",
        "loss_per_fold = []\n",
        "minimal_val_losses = []\n",
        "minimal_val_losses_index = []\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "for train, test in kfold.split(table_of_dataX_learn_normed, table_of_dataY_learn):\n",
        "  model = Sequential()\n",
        "  model.add(layers.Dense(75, input_shape=(280,), activation='relu'))\n",
        "  #model.add(layers.Dense(25,activation='relu',))\n",
        "  model.add(layers.Dense(4))\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='mse'\n",
        "              )\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  print(len(table_of_dataX_learn_normed[train]))\n",
        "  #callback = keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
        "  myval_callback = MyCustomCallback()\n",
        "  history = model.fit(table_of_dataX_learn_normed[train], \n",
        "                    table_of_dataY_learn[train], \n",
        "                    epochs=75,\n",
        "                    verbose=1,\n",
        "                    batch_size = 32,\n",
        "                    callbacks = [myval_callback],\n",
        "                    validation_data = (table_of_dataX_learn_normed[test], table_of_dataY_learn[test])\n",
        "                   )\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Minimal loss epoch and value')\n",
        "  min_loss = np.min(history.history['val_loss'])\n",
        "  minimal_val_losses.append(min_loss)\n",
        "  minimal_val_losses_index.append(np.argmin(history.history['val_loss']))\n",
        "  print(min_loss) \n",
        "  scores = model.evaluate(table_of_dataX_learn_normed[test], table_of_dataY_learn[test], verbose=0)\n",
        "  loss_per_fold.append(scores)\n",
        "  fold_no = fold_no + 1\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(loss_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]}')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "6451\n",
            "Epoch 1/75\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 21.0833 - val_loss: 6.8949\n",
            "6.961720943450928\n",
            "Epoch 2/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 5.9403 - val_loss: 4.9479\n",
            "4.808096408843994\n",
            "Epoch 3/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.6682 - val_loss: 4.5550\n",
            "4.33192777633667\n",
            "Epoch 4/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.3054 - val_loss: 4.3917\n",
            "4.17401647567749\n",
            "Epoch 5/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.2257 - val_loss: 4.3127\n",
            "4.103418827056885\n",
            "Epoch 6/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0744 - val_loss: 4.2864\n",
            "4.089870452880859\n",
            "Epoch 7/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0656 - val_loss: 4.2212\n",
            "4.024020671844482\n",
            "Epoch 8/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9764 - val_loss: 4.1957\n",
            "4.003303527832031\n",
            "Epoch 9/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9971 - val_loss: 4.1685\n",
            "3.971436023712158\n",
            "Epoch 10/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8879 - val_loss: 4.1367\n",
            "3.950183629989624\n",
            "Epoch 11/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9567 - val_loss: 4.1871\n",
            "3.983971357345581\n",
            "Epoch 12/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8522 - val_loss: 4.1264\n",
            "3.9323859214782715\n",
            "Epoch 13/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7993 - val_loss: 4.0740\n",
            "3.8960602283477783\n",
            "Epoch 14/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7637 - val_loss: 4.0877\n",
            "3.908355712890625\n",
            "Epoch 15/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6482 - val_loss: 4.0695\n",
            "3.9081265926361084\n",
            "Epoch 16/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7048 - val_loss: 4.0619\n",
            "3.9137394428253174\n",
            "Epoch 17/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5959 - val_loss: 4.0290\n",
            "3.8350532054901123\n",
            "Epoch 18/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6125 - val_loss: 4.0225\n",
            "3.841418504714966\n",
            "Epoch 19/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5297 - val_loss: 4.0220\n",
            "3.8550734519958496\n",
            "Epoch 20/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5480 - val_loss: 4.0212\n",
            "3.838103771209717\n",
            "Epoch 21/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5318 - val_loss: 3.9810\n",
            "3.7962985038757324\n",
            "Epoch 22/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5097 - val_loss: 3.9948\n",
            "3.807603120803833\n",
            "Epoch 23/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3670 - val_loss: 3.9673\n",
            "3.8057968616485596\n",
            "Epoch 24/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4325 - val_loss: 3.9981\n",
            "3.77689528465271\n",
            "Epoch 25/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4636 - val_loss: 3.9806\n",
            "3.8000388145446777\n",
            "Epoch 26/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4376 - val_loss: 3.9584\n",
            "3.772303342819214\n",
            "Epoch 27/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3575 - val_loss: 4.0034\n",
            "3.790128469467163\n",
            "Epoch 28/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3240 - val_loss: 3.9449\n",
            "3.7484264373779297\n",
            "Epoch 29/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3154 - val_loss: 3.9590\n",
            "3.757310152053833\n",
            "Epoch 30/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3385 - val_loss: 4.0194\n",
            "3.854311466217041\n",
            "Epoch 31/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3104 - val_loss: 3.9663\n",
            "3.762916326522827\n",
            "Epoch 32/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2311 - val_loss: 3.9801\n",
            "3.7618730068206787\n",
            "Epoch 33/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2473 - val_loss: 3.9589\n",
            "3.758556842803955\n",
            "Epoch 34/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2240 - val_loss: 3.9279\n",
            "3.7889273166656494\n",
            "Epoch 35/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1672 - val_loss: 3.9367\n",
            "3.760774850845337\n",
            "Epoch 36/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0689 - val_loss: 3.9566\n",
            "3.7418134212493896\n",
            "Epoch 37/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0987 - val_loss: 3.9828\n",
            "3.7870540618896484\n",
            "Epoch 38/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2174 - val_loss: 3.9619\n",
            "3.7497198581695557\n",
            "Epoch 39/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0293 - val_loss: 3.9255\n",
            "3.722486734390259\n",
            "Epoch 40/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1009 - val_loss: 3.9694\n",
            "3.743683338165283\n",
            "Epoch 41/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0602 - val_loss: 3.9505\n",
            "3.7204432487487793\n",
            "Epoch 42/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0048 - val_loss: 3.9951\n",
            "3.7818031311035156\n",
            "Epoch 43/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0430 - val_loss: 3.9298\n",
            "3.7457642555236816\n",
            "Epoch 44/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0496 - val_loss: 3.9495\n",
            "3.743155002593994\n",
            "Epoch 45/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0304 - val_loss: 3.9152\n",
            "3.7189371585845947\n",
            "Epoch 46/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0126 - val_loss: 3.9387\n",
            "3.7683932781219482\n",
            "Epoch 47/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9619 - val_loss: 3.9495\n",
            "3.7240612506866455\n",
            "Epoch 48/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9446 - val_loss: 3.9133\n",
            "3.7032620906829834\n",
            "Epoch 49/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9653 - val_loss: 3.9632\n",
            "3.7348875999450684\n",
            "Epoch 50/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9429 - val_loss: 3.9349\n",
            "3.7295148372650146\n",
            "Epoch 51/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9343 - val_loss: 3.9433\n",
            "3.7215332984924316\n",
            "Epoch 52/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9049 - val_loss: 3.9409\n",
            "3.7250003814697266\n",
            "Epoch 53/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8567 - val_loss: 3.9589\n",
            "3.75706148147583\n",
            "Epoch 54/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8991 - val_loss: 3.9310\n",
            "3.739316940307617\n",
            "Epoch 55/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8794 - val_loss: 3.9384\n",
            "3.714669704437256\n",
            "Epoch 56/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8328 - val_loss: 3.9647\n",
            "3.7600035667419434\n",
            "Epoch 57/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8509 - val_loss: 3.9365\n",
            "3.719465494155884\n",
            "Epoch 58/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8428 - val_loss: 3.9559\n",
            "3.718508005142212\n",
            "Epoch 59/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8335 - val_loss: 3.9567\n",
            "3.696655511856079\n",
            "Epoch 60/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8749 - val_loss: 3.9664\n",
            "3.743251085281372\n",
            "Epoch 61/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7906 - val_loss: 3.9447\n",
            "3.734163284301758\n",
            "Epoch 62/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7908 - val_loss: 3.9454\n",
            "3.722019672393799\n",
            "Epoch 63/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7891 - val_loss: 3.9676\n",
            "3.6966586112976074\n",
            "Epoch 64/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7826 - val_loss: 4.0072\n",
            "3.764049530029297\n",
            "Epoch 65/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7780 - val_loss: 3.9698\n",
            "3.7395806312561035\n",
            "Epoch 66/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7242 - val_loss: 3.9936\n",
            "3.7548465728759766\n",
            "Epoch 67/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7001 - val_loss: 3.9844\n",
            "3.7826144695281982\n",
            "Epoch 68/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7291 - val_loss: 3.9944\n",
            "3.7740118503570557\n",
            "Epoch 69/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.6991 - val_loss: 3.9953\n",
            "3.7478911876678467\n",
            "Epoch 70/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.6807 - val_loss: 3.9639\n",
            "3.720496654510498\n",
            "Epoch 71/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7294 - val_loss: 4.0153\n",
            "3.7214293479919434\n",
            "Epoch 72/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7089 - val_loss: 4.0129\n",
            "3.741083860397339\n",
            "Epoch 73/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.6574 - val_loss: 3.9870\n",
            "3.74277663230896\n",
            "Epoch 74/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.6038 - val_loss: 4.0039\n",
            "3.764148235321045\n",
            "Epoch 75/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.6024 - val_loss: 4.0032\n",
            "3.7104623317718506\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "3.913254499435425\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "6451\n",
            "Epoch 1/75\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 21.5756 - val_loss: 7.3132\n",
            "7.5543670654296875\n",
            "Epoch 2/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 6.5027 - val_loss: 4.9692\n",
            "5.001785755157471\n",
            "Epoch 3/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.9130 - val_loss: 4.4692\n",
            "4.452669143676758\n",
            "Epoch 4/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.4867 - val_loss: 4.3032\n",
            "4.219883918762207\n",
            "Epoch 5/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.2618 - val_loss: 4.2526\n",
            "4.15803861618042\n",
            "Epoch 6/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.1424 - val_loss: 4.1671\n",
            "4.072582721710205\n",
            "Epoch 7/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.1519 - val_loss: 4.1200\n",
            "4.055070877075195\n",
            "Epoch 8/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0218 - val_loss: 4.0902\n",
            "4.037454128265381\n",
            "Epoch 9/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0400 - val_loss: 4.0528\n",
            "3.9846324920654297\n",
            "Epoch 10/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9882 - val_loss: 4.0512\n",
            "3.978137493133545\n",
            "Epoch 11/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9032 - val_loss: 4.0460\n",
            "4.033276081085205\n",
            "Epoch 12/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8773 - val_loss: 4.0054\n",
            "3.9544005393981934\n",
            "Epoch 13/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8108 - val_loss: 3.9945\n",
            "3.9502856731414795\n",
            "Epoch 14/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7866 - val_loss: 4.0430\n",
            "3.983314275741577\n",
            "Epoch 15/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7798 - val_loss: 3.9786\n",
            "3.929619550704956\n",
            "Epoch 16/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7543 - val_loss: 3.9788\n",
            "3.9200758934020996\n",
            "Epoch 17/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7536 - val_loss: 3.9234\n",
            "3.888488531112671\n",
            "Epoch 18/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6832 - val_loss: 3.9132\n",
            "3.865098714828491\n",
            "Epoch 19/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6269 - val_loss: 3.9266\n",
            "3.8641512393951416\n",
            "Epoch 20/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6684 - val_loss: 3.9001\n",
            "3.8484935760498047\n",
            "Epoch 21/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6196 - val_loss: 3.9173\n",
            "3.842010974884033\n",
            "Epoch 22/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5808 - val_loss: 3.9636\n",
            "3.893569231033325\n",
            "Epoch 23/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5416 - val_loss: 3.8946\n",
            "3.8341121673583984\n",
            "Epoch 24/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4843 - val_loss: 3.9409\n",
            "3.8419299125671387\n",
            "Epoch 25/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6012 - val_loss: 3.9284\n",
            "3.864776372909546\n",
            "Epoch 26/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4331 - val_loss: 3.8826\n",
            "3.8319895267486572\n",
            "Epoch 27/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4490 - val_loss: 3.8764\n",
            "3.781040906906128\n",
            "Epoch 28/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4620 - val_loss: 3.8708\n",
            "3.784205198287964\n",
            "Epoch 29/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3816 - val_loss: 3.9122\n",
            "3.8099000453948975\n",
            "Epoch 30/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3606 - val_loss: 3.8798\n",
            "3.7912497520446777\n",
            "Epoch 31/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3684 - val_loss: 3.8968\n",
            "3.8028550148010254\n",
            "Epoch 32/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3582 - val_loss: 3.8918\n",
            "3.7702624797821045\n",
            "Epoch 33/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3083 - val_loss: 3.8839\n",
            "3.779827356338501\n",
            "Epoch 34/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3370 - val_loss: 3.9069\n",
            "3.7843024730682373\n",
            "Epoch 35/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2735 - val_loss: 3.9211\n",
            "3.8052713871002197\n",
            "Epoch 36/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3011 - val_loss: 3.9119\n",
            "3.7525992393493652\n",
            "Epoch 37/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2668 - val_loss: 3.8934\n",
            "3.764533281326294\n",
            "Epoch 38/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1972 - val_loss: 3.9126\n",
            "3.781717300415039\n",
            "Epoch 39/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2603 - val_loss: 3.9499\n",
            "3.7703475952148438\n",
            "Epoch 40/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1939 - val_loss: 3.9067\n",
            "3.7528321743011475\n",
            "Epoch 41/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1902 - val_loss: 3.9521\n",
            "3.784745693206787\n",
            "Epoch 42/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1360 - val_loss: 3.9431\n",
            "3.7628767490386963\n",
            "Epoch 43/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1447 - val_loss: 4.0050\n",
            "3.7903895378112793\n",
            "Epoch 44/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1407 - val_loss: 4.0091\n",
            "3.821136474609375\n",
            "Epoch 45/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1225 - val_loss: 4.0331\n",
            "3.786093235015869\n",
            "Epoch 46/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0504 - val_loss: 3.9917\n",
            "3.765472173690796\n",
            "Epoch 47/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0542 - val_loss: 4.0033\n",
            "3.736454725265503\n",
            "Epoch 48/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0471 - val_loss: 4.0619\n",
            "3.798879384994507\n",
            "Epoch 49/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0325 - val_loss: 4.0881\n",
            "3.7749078273773193\n",
            "Epoch 50/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0496 - val_loss: 4.0994\n",
            "3.7909491062164307\n",
            "Epoch 51/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9632 - val_loss: 4.1683\n",
            "3.7994401454925537\n",
            "Epoch 52/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0027 - val_loss: 4.1170\n",
            "3.7590744495391846\n",
            "Epoch 53/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9664 - val_loss: 4.1767\n",
            "3.7543880939483643\n",
            "Epoch 54/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9048 - val_loss: 4.1316\n",
            "3.7602977752685547\n",
            "Epoch 55/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0133 - val_loss: 4.2313\n",
            "3.8388540744781494\n",
            "Epoch 56/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8904 - val_loss: 4.1678\n",
            "3.7661755084991455\n",
            "Epoch 57/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9398 - val_loss: 4.1797\n",
            "3.7383015155792236\n",
            "Epoch 58/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8414 - val_loss: 4.1914\n",
            "3.751453161239624\n",
            "Epoch 59/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8920 - val_loss: 4.2744\n",
            "3.7985472679138184\n",
            "Epoch 60/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8893 - val_loss: 4.3011\n",
            "3.829150676727295\n",
            "Epoch 61/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9070 - val_loss: 4.3027\n",
            "3.761509656906128\n",
            "Epoch 62/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8976 - val_loss: 4.2380\n",
            "3.776780605316162\n",
            "Epoch 63/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8376 - val_loss: 4.3140\n",
            "3.761744976043701\n",
            "Epoch 64/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8684 - val_loss: 4.3369\n",
            "3.725797176361084\n",
            "Epoch 65/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7795 - val_loss: 4.3362\n",
            "3.757262706756592\n",
            "Epoch 66/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7896 - val_loss: 4.3872\n",
            "3.7756402492523193\n",
            "Epoch 67/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7450 - val_loss: 4.4530\n",
            "3.811267852783203\n",
            "Epoch 68/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8236 - val_loss: 4.5108\n",
            "3.770275592803955\n",
            "Epoch 69/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7821 - val_loss: 4.4354\n",
            "3.778531551361084\n",
            "Epoch 70/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7694 - val_loss: 4.5867\n",
            "3.7969906330108643\n",
            "Epoch 71/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7855 - val_loss: 4.5122\n",
            "3.7788503170013428\n",
            "Epoch 72/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7584 - val_loss: 4.5989\n",
            "3.763183832168579\n",
            "Epoch 73/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7230 - val_loss: 4.5587\n",
            "3.765294313430786\n",
            "Epoch 74/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7295 - val_loss: 4.6059\n",
            "3.7631676197052\n",
            "Epoch 75/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7265 - val_loss: 4.5751\n",
            "3.7796084880828857\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "3.87083101272583\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "6451\n",
            "Epoch 1/75\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 19.6741 - val_loss: 6.4940\n",
            "6.455524921417236\n",
            "Epoch 2/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 5.6687 - val_loss: 4.8020\n",
            "4.71030330657959\n",
            "Epoch 3/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.6495 - val_loss: 4.4776\n",
            "4.340663433074951\n",
            "Epoch 4/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.3352 - val_loss: 4.3443\n",
            "4.195143222808838\n",
            "Epoch 5/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.2484 - val_loss: 4.2381\n",
            "4.107226848602295\n",
            "Epoch 6/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.1565 - val_loss: 4.1925\n",
            "4.074252605438232\n",
            "Epoch 7/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.1145 - val_loss: 4.1542\n",
            "4.065908908843994\n",
            "Epoch 8/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0314 - val_loss: 4.2186\n",
            "4.096500873565674\n",
            "Epoch 9/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0474 - val_loss: 4.0908\n",
            "4.0011091232299805\n",
            "Epoch 10/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0160 - val_loss: 4.1147\n",
            "4.0392961502075195\n",
            "Epoch 11/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8963 - val_loss: 4.0735\n",
            "3.9747135639190674\n",
            "Epoch 12/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9302 - val_loss: 4.0671\n",
            "3.9666054248809814\n",
            "Epoch 13/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8701 - val_loss: 4.1120\n",
            "4.039494514465332\n",
            "Epoch 14/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8656 - val_loss: 4.0215\n",
            "3.925856828689575\n",
            "Epoch 15/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7762 - val_loss: 4.0468\n",
            "3.9416747093200684\n",
            "Epoch 16/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7880 - val_loss: 4.0179\n",
            "3.9131641387939453\n",
            "Epoch 17/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7466 - val_loss: 4.0038\n",
            "3.8904178142547607\n",
            "Epoch 18/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7900 - val_loss: 3.9908\n",
            "3.8968892097473145\n",
            "Epoch 19/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6340 - val_loss: 3.9762\n",
            "3.882112503051758\n",
            "Epoch 20/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6779 - val_loss: 4.0357\n",
            "3.914759397506714\n",
            "Epoch 21/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6248 - val_loss: 3.9723\n",
            "3.8690524101257324\n",
            "Epoch 22/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6410 - val_loss: 3.9904\n",
            "3.8729660511016846\n",
            "Epoch 23/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6550 - val_loss: 3.9547\n",
            "3.842212677001953\n",
            "Epoch 24/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5723 - val_loss: 3.9679\n",
            "3.8436787128448486\n",
            "Epoch 25/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5694 - val_loss: 3.9631\n",
            "3.8338356018066406\n",
            "Epoch 26/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5755 - val_loss: 3.9505\n",
            "3.8306310176849365\n",
            "Epoch 27/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4965 - val_loss: 3.9817\n",
            "3.842703104019165\n",
            "Epoch 28/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4784 - val_loss: 3.9684\n",
            "3.8355770111083984\n",
            "Epoch 29/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4929 - val_loss: 3.9296\n",
            "3.8150508403778076\n",
            "Epoch 30/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4793 - val_loss: 3.9412\n",
            "3.8241119384765625\n",
            "Epoch 31/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3980 - val_loss: 3.9734\n",
            "3.853911876678467\n",
            "Epoch 32/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4861 - val_loss: 3.9179\n",
            "3.8057682514190674\n",
            "Epoch 33/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4097 - val_loss: 3.9403\n",
            "3.8111932277679443\n",
            "Epoch 34/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3973 - val_loss: 3.9492\n",
            "3.8008313179016113\n",
            "Epoch 35/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3787 - val_loss: 3.9386\n",
            "3.8094780445098877\n",
            "Epoch 36/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4083 - val_loss: 3.9180\n",
            "3.774200439453125\n",
            "Epoch 37/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3060 - val_loss: 3.9099\n",
            "3.789428949356079\n",
            "Epoch 38/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3547 - val_loss: 3.9136\n",
            "3.768648862838745\n",
            "Epoch 39/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3261 - val_loss: 3.8940\n",
            "3.7756125926971436\n",
            "Epoch 40/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2453 - val_loss: 3.9256\n",
            "3.789958953857422\n",
            "Epoch 41/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3272 - val_loss: 3.9269\n",
            "3.7763893604278564\n",
            "Epoch 42/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3027 - val_loss: 3.8777\n",
            "3.7556545734405518\n",
            "Epoch 43/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3081 - val_loss: 3.9050\n",
            "3.765188455581665\n",
            "Epoch 44/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2468 - val_loss: 3.9317\n",
            "3.8172857761383057\n",
            "Epoch 45/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2521 - val_loss: 3.9495\n",
            "3.7899625301361084\n",
            "Epoch 46/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2006 - val_loss: 3.8869\n",
            "3.757978677749634\n",
            "Epoch 47/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2243 - val_loss: 3.8777\n",
            "3.7772433757781982\n",
            "Epoch 48/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1750 - val_loss: 3.9302\n",
            "3.809194803237915\n",
            "Epoch 49/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1403 - val_loss: 3.8724\n",
            "3.737067461013794\n",
            "Epoch 50/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1554 - val_loss: 3.9078\n",
            "3.8171489238739014\n",
            "Epoch 51/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1327 - val_loss: 3.8822\n",
            "3.781132459640503\n",
            "Epoch 52/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1125 - val_loss: 3.9127\n",
            "3.7789511680603027\n",
            "Epoch 53/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1330 - val_loss: 3.8757\n",
            "3.7701282501220703\n",
            "Epoch 54/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1249 - val_loss: 3.8957\n",
            "3.7484843730926514\n",
            "Epoch 55/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0283 - val_loss: 3.9144\n",
            "3.795133113861084\n",
            "Epoch 56/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1186 - val_loss: 3.8921\n",
            "3.7874464988708496\n",
            "Epoch 57/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1170 - val_loss: 3.8823\n",
            "3.7458336353302\n",
            "Epoch 58/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0191 - val_loss: 3.8677\n",
            "3.766843557357788\n",
            "Epoch 59/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0327 - val_loss: 3.8879\n",
            "3.8137621879577637\n",
            "Epoch 60/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0284 - val_loss: 3.9215\n",
            "3.765864610671997\n",
            "Epoch 61/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0480 - val_loss: 3.8676\n",
            "3.756633996963501\n",
            "Epoch 62/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0526 - val_loss: 3.8801\n",
            "3.759028196334839\n",
            "Epoch 63/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0127 - val_loss: 3.9404\n",
            "3.8334102630615234\n",
            "Epoch 64/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9866 - val_loss: 3.8883\n",
            "3.7787959575653076\n",
            "Epoch 65/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0622 - val_loss: 3.8751\n",
            "3.779724597930908\n",
            "Epoch 66/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0114 - val_loss: 3.8654\n",
            "3.7755277156829834\n",
            "Epoch 67/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9724 - val_loss: 3.9005\n",
            "3.7441890239715576\n",
            "Epoch 68/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9326 - val_loss: 3.9164\n",
            "3.830026149749756\n",
            "Epoch 69/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0126 - val_loss: 3.8358\n",
            "3.7809715270996094\n",
            "Epoch 70/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9379 - val_loss: 3.8517\n",
            "3.7115046977996826\n",
            "Epoch 71/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9623 - val_loss: 3.8819\n",
            "3.77040433883667\n",
            "Epoch 72/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9082 - val_loss: 3.8837\n",
            "3.7939038276672363\n",
            "Epoch 73/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8898 - val_loss: 3.8765\n",
            "3.802318811416626\n",
            "Epoch 74/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8949 - val_loss: 3.8834\n",
            "3.7819981575012207\n",
            "Epoch 75/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8568 - val_loss: 3.8695\n",
            "3.7545206546783447\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "3.8358335494995117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "6451\n",
            "Epoch 1/75\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 20.3804 - val_loss: 7.2485\n",
            "7.1121063232421875\n",
            "Epoch 2/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 6.2172 - val_loss: 5.0343\n",
            "4.904622554779053\n",
            "Epoch 3/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.7982 - val_loss: 4.6306\n",
            "4.437254905700684\n",
            "Epoch 4/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.4412 - val_loss: 4.4120\n",
            "4.20725679397583\n",
            "Epoch 5/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.2641 - val_loss: 4.3382\n",
            "4.147569179534912\n",
            "Epoch 6/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.1698 - val_loss: 4.2641\n",
            "4.075401306152344\n",
            "Epoch 7/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.1121 - val_loss: 4.2218\n",
            "4.043924331665039\n",
            "Epoch 8/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0511 - val_loss: 4.1687\n",
            "4.024538040161133\n",
            "Epoch 9/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9529 - val_loss: 4.1393\n",
            "4.01522970199585\n",
            "Epoch 10/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9504 - val_loss: 4.1635\n",
            "3.9946892261505127\n",
            "Epoch 11/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0041 - val_loss: 4.1182\n",
            "3.9593679904937744\n",
            "Epoch 12/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8126 - val_loss: 4.1048\n",
            "3.963106393814087\n",
            "Epoch 13/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8447 - val_loss: 4.0599\n",
            "3.930851459503174\n",
            "Epoch 14/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8368 - val_loss: 4.0386\n",
            "3.918947696685791\n",
            "Epoch 15/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8431 - val_loss: 4.0380\n",
            "3.903486967086792\n",
            "Epoch 16/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7352 - val_loss: 4.0437\n",
            "3.8865742683410645\n",
            "Epoch 17/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7417 - val_loss: 4.0350\n",
            "3.856931209564209\n",
            "Epoch 18/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7485 - val_loss: 4.0061\n",
            "3.900221347808838\n",
            "Epoch 19/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6336 - val_loss: 4.0272\n",
            "3.8302226066589355\n",
            "Epoch 20/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6023 - val_loss: 4.0073\n",
            "3.8372855186462402\n",
            "Epoch 21/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6267 - val_loss: 4.0335\n",
            "3.870680332183838\n",
            "Epoch 22/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5677 - val_loss: 3.9702\n",
            "3.817704677581787\n",
            "Epoch 23/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5306 - val_loss: 3.9982\n",
            "3.8158771991729736\n",
            "Epoch 24/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4539 - val_loss: 3.9406\n",
            "3.796421766281128\n",
            "Epoch 25/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4466 - val_loss: 3.9372\n",
            "3.7741446495056152\n",
            "Epoch 26/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4366 - val_loss: 3.9302\n",
            "3.789414644241333\n",
            "Epoch 27/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4234 - val_loss: 3.9464\n",
            "3.7834341526031494\n",
            "Epoch 28/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4572 - val_loss: 3.9267\n",
            "3.795280933380127\n",
            "Epoch 29/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4460 - val_loss: 3.9580\n",
            "3.7987289428710938\n",
            "Epoch 30/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4080 - val_loss: 3.9599\n",
            "3.751878499984741\n",
            "Epoch 31/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3457 - val_loss: 3.9514\n",
            "3.762342691421509\n",
            "Epoch 32/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3079 - val_loss: 3.9200\n",
            "3.7508528232574463\n",
            "Epoch 33/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3363 - val_loss: 3.9334\n",
            "3.7193472385406494\n",
            "Epoch 34/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3509 - val_loss: 3.9168\n",
            "3.749438524246216\n",
            "Epoch 35/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3178 - val_loss: 3.9444\n",
            "3.7358479499816895\n",
            "Epoch 36/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3390 - val_loss: 3.9390\n",
            "3.7359931468963623\n",
            "Epoch 37/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2887 - val_loss: 3.9614\n",
            "3.760823965072632\n",
            "Epoch 38/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1210 - val_loss: 3.9610\n",
            "3.7566239833831787\n",
            "Epoch 39/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2499 - val_loss: 3.9740\n",
            "3.7805049419403076\n",
            "Epoch 40/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1987 - val_loss: 3.9296\n",
            "3.7393548488616943\n",
            "Epoch 41/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2249 - val_loss: 3.9075\n",
            "3.704967975616455\n",
            "Epoch 42/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1920 - val_loss: 3.9360\n",
            "3.6976687908172607\n",
            "Epoch 43/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1606 - val_loss: 3.9090\n",
            "3.7008984088897705\n",
            "Epoch 44/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1175 - val_loss: 3.9484\n",
            "3.7520949840545654\n",
            "Epoch 45/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1378 - val_loss: 3.9184\n",
            "3.723377227783203\n",
            "Epoch 46/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0874 - val_loss: 3.9553\n",
            "3.731534242630005\n",
            "Epoch 47/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0769 - val_loss: 4.0211\n",
            "3.7456443309783936\n",
            "Epoch 48/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1054 - val_loss: 3.9115\n",
            "3.7250120639801025\n",
            "Epoch 49/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0432 - val_loss: 3.9683\n",
            "3.742173671722412\n",
            "Epoch 50/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0426 - val_loss: 3.9312\n",
            "3.710597515106201\n",
            "Epoch 51/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0550 - val_loss: 3.9479\n",
            "3.7662546634674072\n",
            "Epoch 52/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0314 - val_loss: 3.9594\n",
            "3.74895977973938\n",
            "Epoch 53/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9929 - val_loss: 3.9330\n",
            "3.707702398300171\n",
            "Epoch 54/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9594 - val_loss: 3.9676\n",
            "3.722771406173706\n",
            "Epoch 55/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9621 - val_loss: 3.9738\n",
            "3.752415418624878\n",
            "Epoch 56/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9018 - val_loss: 3.9179\n",
            "3.728970766067505\n",
            "Epoch 57/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9891 - val_loss: 3.9450\n",
            "3.74817156791687\n",
            "Epoch 58/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9329 - val_loss: 3.9382\n",
            "3.725055694580078\n",
            "Epoch 59/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9253 - val_loss: 3.9721\n",
            "3.780076503753662\n",
            "Epoch 60/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8433 - val_loss: 3.9871\n",
            "3.7189505100250244\n",
            "Epoch 61/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9454 - val_loss: 3.9460\n",
            "3.709977388381958\n",
            "Epoch 62/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9272 - val_loss: 3.9421\n",
            "3.74625825881958\n",
            "Epoch 63/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9060 - val_loss: 3.9312\n",
            "3.7215068340301514\n",
            "Epoch 64/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8531 - val_loss: 3.9496\n",
            "3.741384983062744\n",
            "Epoch 65/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8464 - val_loss: 3.9483\n",
            "3.7713661193847656\n",
            "Epoch 66/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8129 - val_loss: 3.9566\n",
            "3.7401206493377686\n",
            "Epoch 67/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8608 - val_loss: 3.9699\n",
            "3.798733949661255\n",
            "Epoch 68/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8110 - val_loss: 3.9795\n",
            "3.752756357192993\n",
            "Epoch 69/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7324 - val_loss: 3.9676\n",
            "3.705343008041382\n",
            "Epoch 70/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8303 - val_loss: 3.9947\n",
            "3.7367501258850098\n",
            "Epoch 71/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7870 - val_loss: 3.9418\n",
            "3.762348175048828\n",
            "Epoch 72/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7956 - val_loss: 4.0095\n",
            "3.752025604248047\n",
            "Epoch 73/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8228 - val_loss: 3.9686\n",
            "3.7299129962921143\n",
            "Epoch 74/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7255 - val_loss: 3.9594\n",
            "3.7489659786224365\n",
            "Epoch 75/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7676 - val_loss: 3.9805\n",
            "3.781954050064087\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "3.907503604888916\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "6452\n",
            "Epoch 1/75\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 21.3515 - val_loss: 7.5097\n",
            "7.36655330657959\n",
            "Epoch 2/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 6.3163 - val_loss: 5.1619\n",
            "5.005279541015625\n",
            "Epoch 3/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.8582 - val_loss: 4.6173\n",
            "4.431590557098389\n",
            "Epoch 4/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.4007 - val_loss: 4.4360\n",
            "4.244234561920166\n",
            "Epoch 5/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.2881 - val_loss: 4.3329\n",
            "4.149983882904053\n",
            "Epoch 6/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.2022 - val_loss: 4.2645\n",
            "4.086335182189941\n",
            "Epoch 7/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0801 - val_loss: 4.2524\n",
            "4.042240619659424\n",
            "Epoch 8/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 4.0747 - val_loss: 4.2198\n",
            "4.017300128936768\n",
            "Epoch 9/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9270 - val_loss: 4.1749\n",
            "4.001550197601318\n",
            "Epoch 10/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9465 - val_loss: 4.1831\n",
            "3.9931271076202393\n",
            "Epoch 11/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.9185 - val_loss: 4.1340\n",
            "3.9509708881378174\n",
            "Epoch 12/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8760 - val_loss: 4.1150\n",
            "3.9554786682128906\n",
            "Epoch 13/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.8036 - val_loss: 4.0814\n",
            "3.904715061187744\n",
            "Epoch 14/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7058 - val_loss: 4.1073\n",
            "3.8997128009796143\n",
            "Epoch 15/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6702 - val_loss: 4.1044\n",
            "3.9157586097717285\n",
            "Epoch 16/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7044 - val_loss: 4.0677\n",
            "3.9193532466888428\n",
            "Epoch 17/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6766 - val_loss: 4.0805\n",
            "3.8863584995269775\n",
            "Epoch 18/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.6444 - val_loss: 4.0153\n",
            "3.898211717605591\n",
            "Epoch 19/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.7056 - val_loss: 4.0285\n",
            "3.863129138946533\n",
            "Epoch 20/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5536 - val_loss: 4.0354\n",
            "3.873603105545044\n",
            "Epoch 21/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5997 - val_loss: 4.0364\n",
            "3.8562240600585938\n",
            "Epoch 22/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5094 - val_loss: 4.0264\n",
            "3.875368595123291\n",
            "Epoch 23/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.5412 - val_loss: 4.0749\n",
            "3.85971999168396\n",
            "Epoch 24/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4405 - val_loss: 4.0267\n",
            "3.8644964694976807\n",
            "Epoch 25/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4718 - val_loss: 4.0412\n",
            "3.8803579807281494\n",
            "Epoch 26/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4918 - val_loss: 4.0123\n",
            "3.8408334255218506\n",
            "Epoch 27/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4392 - val_loss: 4.0647\n",
            "3.891519069671631\n",
            "Epoch 28/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3484 - val_loss: 3.9676\n",
            "3.8541221618652344\n",
            "Epoch 29/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.4020 - val_loss: 4.0177\n",
            "3.901994466781616\n",
            "Epoch 30/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3173 - val_loss: 4.1071\n",
            "3.9267632961273193\n",
            "Epoch 31/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.3536 - val_loss: 4.0532\n",
            "3.837756633758545\n",
            "Epoch 32/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2594 - val_loss: 3.9968\n",
            "3.873701572418213\n",
            "Epoch 33/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2933 - val_loss: 3.9806\n",
            "3.851710081100464\n",
            "Epoch 34/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1981 - val_loss: 4.0180\n",
            "3.8611109256744385\n",
            "Epoch 35/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2416 - val_loss: 4.0409\n",
            "3.882392168045044\n",
            "Epoch 36/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.2637 - val_loss: 3.9942\n",
            "3.863865375518799\n",
            "Epoch 37/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1804 - val_loss: 4.0385\n",
            "3.847992181777954\n",
            "Epoch 38/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1572 - val_loss: 4.0200\n",
            "3.8489317893981934\n",
            "Epoch 39/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1885 - val_loss: 4.0294\n",
            "3.863603353500366\n",
            "Epoch 40/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1395 - val_loss: 4.0076\n",
            "3.8675386905670166\n",
            "Epoch 41/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1179 - val_loss: 4.0380\n",
            "3.842808961868286\n",
            "Epoch 42/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.1340 - val_loss: 4.0105\n",
            "3.876993179321289\n",
            "Epoch 43/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0935 - val_loss: 4.0470\n",
            "3.8678154945373535\n",
            "Epoch 44/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0433 - val_loss: 4.0278\n",
            "3.8979218006134033\n",
            "Epoch 45/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0776 - val_loss: 4.0962\n",
            "3.9722800254821777\n",
            "Epoch 46/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0406 - val_loss: 4.1094\n",
            "3.8886945247650146\n",
            "Epoch 47/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0006 - val_loss: 4.0513\n",
            "3.8577370643615723\n",
            "Epoch 48/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0192 - val_loss: 4.0737\n",
            "3.914980173110962\n",
            "Epoch 49/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0049 - val_loss: 4.1331\n",
            "3.8976027965545654\n",
            "Epoch 50/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 3.0347 - val_loss: 4.1173\n",
            "3.869459867477417\n",
            "Epoch 51/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9369 - val_loss: 4.1062\n",
            "3.8656270503997803\n",
            "Epoch 52/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9154 - val_loss: 4.1600\n",
            "3.9147822856903076\n",
            "Epoch 53/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9797 - val_loss: 4.1262\n",
            "3.9237112998962402\n",
            "Epoch 54/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9104 - val_loss: 4.1177\n",
            "3.8935792446136475\n",
            "Epoch 55/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8962 - val_loss: 4.1482\n",
            "3.956463098526001\n",
            "Epoch 56/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9434 - val_loss: 4.1855\n",
            "3.892481565475464\n",
            "Epoch 57/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.9002 - val_loss: 4.2169\n",
            "3.9409291744232178\n",
            "Epoch 58/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8721 - val_loss: 4.1469\n",
            "3.9051342010498047\n",
            "Epoch 59/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8606 - val_loss: 4.2166\n",
            "3.9009151458740234\n",
            "Epoch 60/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8381 - val_loss: 4.2040\n",
            "3.8869335651397705\n",
            "Epoch 61/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8041 - val_loss: 4.1825\n",
            "3.903972625732422\n",
            "Epoch 62/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7873 - val_loss: 4.2117\n",
            "3.9196791648864746\n",
            "Epoch 63/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8179 - val_loss: 4.2133\n",
            "3.8988680839538574\n",
            "Epoch 64/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7661 - val_loss: 4.2730\n",
            "3.93072509765625\n",
            "Epoch 65/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8237 - val_loss: 4.2319\n",
            "3.9071576595306396\n",
            "Epoch 66/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7419 - val_loss: 4.3217\n",
            "3.9014265537261963\n",
            "Epoch 67/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.8139 - val_loss: 4.2582\n",
            "3.913109302520752\n",
            "Epoch 68/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7749 - val_loss: 4.3165\n",
            "3.9320435523986816\n",
            "Epoch 69/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7406 - val_loss: 4.3223\n",
            "3.9166367053985596\n",
            "Epoch 70/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7247 - val_loss: 4.3402\n",
            "3.9263815879821777\n",
            "Epoch 71/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.6724 - val_loss: 4.4161\n",
            "3.914591073989868\n",
            "Epoch 72/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.7126 - val_loss: 4.3247\n",
            "3.923067808151245\n",
            "Epoch 73/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.6483 - val_loss: 4.4428\n",
            "3.930138349533081\n",
            "Epoch 74/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.6610 - val_loss: 4.3982\n",
            "3.9632492065429688\n",
            "Epoch 75/75\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 2.6663 - val_loss: 4.4062\n",
            "3.926604747772217\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "3.967649459838867\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 4.003172874450684\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 4.575074672698975\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 3.8695225715637207\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 3.980508327484131\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 4.406204700469971\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Loss: 4.166896629333496\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjDe0faZP8W-",
        "outputId": "cad5c797-a4df-44a7-f536-233f422787db"
      },
      "source": [
        "print(minimal_val_losses_index)\n",
        "print(minimal_val_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[47, 27, 68, 40, 27]\n",
            "[3.913254499435425, 3.87083101272583, 3.8358335494995117, 3.907503604888916, 3.967649459838867]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}