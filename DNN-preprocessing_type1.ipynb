{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVEX5B5IjnqJ"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9YJSmUVR_Tf",
        "outputId": "ddaa440b-916d-4073-d1b0-a97f2a49aec1"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R23Bet1Qmf8S"
      },
      "source": [
        "files = []\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s01.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s02.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s03.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s04.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s05.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s06.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s07.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s08.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s09.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s10.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s11.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s12.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s13.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s14.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s15.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s16.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s17.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s18.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s19.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s20.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s21.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s22.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s23.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s24.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s25.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s26.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s27.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s28.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s29.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s30.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s31.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s32.dat')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3CmentroEDU"
      },
      "source": [
        "# wczytanie danych i preprocessing 1\n",
        "table_of_dataX = []\n",
        "table_of_dataY = []\n",
        "for i in range(32):\n",
        "  f = files[i]\n",
        "  with open(f, 'rb') as f: content = pickle.load(f, encoding='latin1')\n",
        "  data = content['data']\n",
        "  labels = content['labels']\n",
        "  for j in range(40): #40 filmow\n",
        "    vector_input = []\n",
        "    tmpX = data[j]\n",
        "    tmpY = labels[j]\n",
        "    table_of_dataY.append(tmpY)\n",
        "    for k in range(40): #40 sygnalow\n",
        "      mean = np.mean(tmpX[k])\n",
        "      median = np.median(tmpX[k])\n",
        "      max = np.max(tmpX[k])\n",
        "      min = np.min(tmpX[k])\n",
        "      variance = np.var(tmpX[k])\n",
        "      skewness = skew(tmpX[k])\n",
        "      kur = kurtosis(tmpX[k])\n",
        "      vector_input.append(mean)\n",
        "      vector_input.append(median)\n",
        "      vector_input.append(max)\n",
        "      vector_input.append(min)\n",
        "      vector_input.append(variance)\n",
        "      vector_input.append(skewness)\n",
        "      vector_input.append(kur)\n",
        "    table_of_dataX.append(vector_input)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4I9C_wPtTyj"
      },
      "source": [
        "table_of_dataX = np.asarray(table_of_dataX)\n",
        "table_of_dataY = np.asarray(table_of_dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHzPV3vpnlSe"
      },
      "source": [
        "randnums = [   4,    5,   26,   34,   36,   52,   56,   57,   72,   80,   87,\n",
        "         92,  119,  134,  141,  155,  165,  197,  228,  233,  239,  289,\n",
        "        291,  293,  311,  315,  318,  334,  371,  375,  383,  416,  434,\n",
        "        435,  442,  444,  450,  471,  472,  475,  502,  504,  509,  516,\n",
        "        540,  549,  550,  555,  587,  603,  622,  623,  635,  648,  676,\n",
        "        692,  693,  694,  702,  716,  732,  745,  754,  758,  760,  764,\n",
        "        772,  773,  778,  779,  787,  814,  815,  817,  846,  850,  865,\n",
        "        872,  876,  881,  889,  895,  901,  935,  936,  963,  976,  984,\n",
        "        991,  992,  996, 1010, 1013, 1017, 1020, 1060, 1067, 1070, 1071,\n",
        "       1087, 1088, 1091, 1098, 1118, 1122, 1130, 1148, 1150, 1166, 1171,\n",
        "       1187, 1197, 1213, 1219, 1225, 1240, 1259, 1260, 1264, 1271]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7lWZYOWLXC_"
      },
      "source": [
        "testdataX = []\n",
        "testdataY = []\n",
        "for i in randnums:\n",
        "  testdataX.append(table_of_dataX[i])\n",
        "  testdataY.append(table_of_dataY[i])\n",
        "table_of_dataX_learn = np.delete(table_of_dataX, randnums, 0)\n",
        "table_of_dataY_learn = np.delete(table_of_dataY, randnums, 0)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASPEqxuzrohL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363818b2-9151-478c-a7e4-4f94d0553977"
      },
      "source": [
        "divider_array = np.max(np.abs(table_of_dataX_learn), axis=0)\n",
        "table_of_dataX_learn_normed = table_of_dataX_learn/divider_array\n",
        "testdataX_normed = testdataX/divider_array\n",
        "testdataY = np.asarray(testdataY)\n",
        "len(table_of_dataX_learn_normed)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1160"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxukLCdBFaw4"
      },
      "source": [
        "\n",
        "class MyCustomCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        res_eval_1 = self.model.evaluate(testdataX_normed, testdataY, verbose = 0)\n",
        "        print(res_eval_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2yLbQoUSCwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f980cf52-946e-49bb-956c-3d39ae0d31a3"
      },
      "source": [
        "num_folds = 5\n",
        "fold_no = 1\n",
        "loss_per_fold = []\n",
        "minimal_val_losses = []\n",
        "minimal_val_losses_index = []\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "for train, test in kfold.split(table_of_dataX_learn_normed, table_of_dataY_learn):\n",
        "  model = Sequential()\n",
        "  model.add(layers.Dense(70, input_shape=(280,), activation='relu'))\n",
        "  model.add(layers.Dense(25,activation='relu',))\n",
        "  model.add(layers.Dense(4))\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='mse'\n",
        "              )\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  print(len(table_of_dataX_learn_normed[train]))\n",
        "  #callback = keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
        "  myval_callback = MyCustomCallback()\n",
        "  history = model.fit(table_of_dataX_learn_normed[train], \n",
        "                    table_of_dataY_learn[train], \n",
        "                    epochs=75,\n",
        "                    verbose=1,\n",
        "                    batch_size = 32,\n",
        "                    callbacks = [myval_callback],\n",
        "                    validation_data = (table_of_dataX_learn_normed[test], table_of_dataY_learn[test])\n",
        "                   )\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Minimal loss epoch and value')\n",
        "  min_loss = np.min(history.history['val_loss'])\n",
        "  minimal_val_losses.append(min_loss)\n",
        "  minimal_val_losses_index.append(np.argmin(history.history['val_loss']))\n",
        "  print(min_loss) \n",
        "  scores = model.evaluate(table_of_dataX_learn_normed[test], table_of_dataY_learn[test], verbose=0)\n",
        "  loss_per_fold.append(scores)\n",
        "  fold_no = fold_no + 1\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(loss_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]}')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "928\n",
            "Epoch 1/75\n",
            "29/29 [==============================] - 1s 12ms/step - loss: 30.8853 - val_loss: 20.5815\n",
            "19.379291534423828\n",
            "Epoch 2/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 16.0684 - val_loss: 10.5039\n",
            "13.057282447814941\n",
            "Epoch 3/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 11.0218 - val_loss: 8.1281\n",
            "9.768738746643066\n",
            "Epoch 4/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 8.0163 - val_loss: 6.8006\n",
            "8.481188774108887\n",
            "Epoch 5/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 6.7311 - val_loss: 6.0072\n",
            "7.666383743286133\n",
            "Epoch 6/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 6.1498 - val_loss: 5.4835\n",
            "7.093696117401123\n",
            "Epoch 7/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 5.6440 - val_loss: 5.1408\n",
            "6.629098415374756\n",
            "Epoch 8/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 5.0614 - val_loss: 4.9664\n",
            "6.242798805236816\n",
            "Epoch 9/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.8742 - val_loss: 4.6561\n",
            "6.071755409240723\n",
            "Epoch 10/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.4376 - val_loss: 4.5871\n",
            "5.79644775390625\n",
            "Epoch 11/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.6673 - val_loss: 4.3454\n",
            "5.891334056854248\n",
            "Epoch 12/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.5955 - val_loss: 4.3019\n",
            "5.634135723114014\n",
            "Epoch 13/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.1028 - val_loss: 4.2548\n",
            "5.652074337005615\n",
            "Epoch 14/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.2627 - val_loss: 4.2040\n",
            "5.440168380737305\n",
            "Epoch 15/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.3029 - val_loss: 4.1870\n",
            "5.499026298522949\n",
            "Epoch 16/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.1582 - val_loss: 4.2044\n",
            "5.318655490875244\n",
            "Epoch 17/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.9865 - val_loss: 4.2979\n",
            "5.2061896324157715\n",
            "Epoch 18/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.1483 - val_loss: 4.0943\n",
            "5.339134216308594\n",
            "Epoch 19/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8067 - val_loss: 4.1242\n",
            "5.268991947174072\n",
            "Epoch 20/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7797 - val_loss: 4.0626\n",
            "5.329320430755615\n",
            "Epoch 21/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7641 - val_loss: 4.0919\n",
            "5.296024799346924\n",
            "Epoch 22/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8471 - val_loss: 4.1262\n",
            "5.1308722496032715\n",
            "Epoch 23/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7262 - val_loss: 4.0876\n",
            "5.193228721618652\n",
            "Epoch 24/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7838 - val_loss: 4.1257\n",
            "5.068159103393555\n",
            "Epoch 25/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8479 - val_loss: 4.0783\n",
            "5.1776509284973145\n",
            "Epoch 26/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7137 - val_loss: 4.1995\n",
            "5.032255172729492\n",
            "Epoch 27/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.4769 - val_loss: 4.2122\n",
            "5.011989116668701\n",
            "Epoch 28/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4788 - val_loss: 4.0977\n",
            "5.141603469848633\n",
            "Epoch 29/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.3699 - val_loss: 4.1698\n",
            "5.087993144989014\n",
            "Epoch 30/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3260 - val_loss: 4.0979\n",
            "5.287516117095947\n",
            "Epoch 31/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4981 - val_loss: 4.1347\n",
            "5.2437663078308105\n",
            "Epoch 32/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3736 - val_loss: 4.1181\n",
            "5.2170939445495605\n",
            "Epoch 33/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3148 - val_loss: 4.1236\n",
            "5.271938800811768\n",
            "Epoch 34/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4253 - val_loss: 4.1284\n",
            "5.3444294929504395\n",
            "Epoch 35/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3563 - val_loss: 4.1431\n",
            "5.465794086456299\n",
            "Epoch 36/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2186 - val_loss: 4.1816\n",
            "5.333144187927246\n",
            "Epoch 37/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3910 - val_loss: 4.2175\n",
            "5.360547065734863\n",
            "Epoch 38/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2005 - val_loss: 4.1913\n",
            "5.6478447914123535\n",
            "Epoch 39/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2476 - val_loss: 4.2794\n",
            "5.359660625457764\n",
            "Epoch 40/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.2346 - val_loss: 4.2031\n",
            "5.618592739105225\n",
            "Epoch 41/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2531 - val_loss: 4.2602\n",
            "5.543581962585449\n",
            "Epoch 42/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.0289 - val_loss: 4.2728\n",
            "5.566147804260254\n",
            "Epoch 43/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9699 - val_loss: 4.2946\n",
            "5.635893821716309\n",
            "Epoch 44/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.0038 - val_loss: 4.3459\n",
            "5.73252010345459\n",
            "Epoch 45/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9286 - val_loss: 4.2865\n",
            "5.614001274108887\n",
            "Epoch 46/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8411 - val_loss: 4.3268\n",
            "5.839699745178223\n",
            "Epoch 47/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9308 - val_loss: 4.3376\n",
            "5.919503211975098\n",
            "Epoch 48/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7812 - val_loss: 4.3603\n",
            "5.913116455078125\n",
            "Epoch 49/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.7624 - val_loss: 4.3670\n",
            "5.862680435180664\n",
            "Epoch 50/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.7929 - val_loss: 4.4381\n",
            "5.866857528686523\n",
            "Epoch 51/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7569 - val_loss: 4.5131\n",
            "5.9731621742248535\n",
            "Epoch 52/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.6388 - val_loss: 4.4413\n",
            "6.029560565948486\n",
            "Epoch 53/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6779 - val_loss: 4.4245\n",
            "6.168973922729492\n",
            "Epoch 54/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6227 - val_loss: 4.4674\n",
            "6.111128807067871\n",
            "Epoch 55/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6442 - val_loss: 4.5197\n",
            "6.045343399047852\n",
            "Epoch 56/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6297 - val_loss: 4.5563\n",
            "6.1908440589904785\n",
            "Epoch 57/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6698 - val_loss: 4.4901\n",
            "6.287121772766113\n",
            "Epoch 58/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5150 - val_loss: 4.5129\n",
            "6.394493579864502\n",
            "Epoch 59/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5264 - val_loss: 4.5505\n",
            "6.2719879150390625\n",
            "Epoch 60/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.4854 - val_loss: 4.6953\n",
            "6.4536004066467285\n",
            "Epoch 61/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5299 - val_loss: 4.5894\n",
            "6.497633457183838\n",
            "Epoch 62/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5057 - val_loss: 4.7282\n",
            "6.323679447174072\n",
            "Epoch 63/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4547 - val_loss: 4.7189\n",
            "6.45054292678833\n",
            "Epoch 64/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3799 - val_loss: 4.6135\n",
            "6.632078170776367\n",
            "Epoch 65/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3183 - val_loss: 4.6814\n",
            "6.540560722351074\n",
            "Epoch 66/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3389 - val_loss: 4.6384\n",
            "6.794181823730469\n",
            "Epoch 67/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4151 - val_loss: 4.7007\n",
            "6.926287651062012\n",
            "Epoch 68/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3094 - val_loss: 4.6519\n",
            "6.925153732299805\n",
            "Epoch 69/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3412 - val_loss: 4.7370\n",
            "6.8915276527404785\n",
            "Epoch 70/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3525 - val_loss: 4.7419\n",
            "6.862525463104248\n",
            "Epoch 71/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.2924 - val_loss: 4.7516\n",
            "7.14071798324585\n",
            "Epoch 72/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2314 - val_loss: 4.8086\n",
            "7.21104621887207\n",
            "Epoch 73/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2382 - val_loss: 4.7856\n",
            "7.127043724060059\n",
            "Epoch 74/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.2485 - val_loss: 4.8976\n",
            "7.528258323669434\n",
            "Epoch 75/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.1527 - val_loss: 4.9076\n",
            "7.3758625984191895\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "4.0626349449157715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "928\n",
            "Epoch 1/75\n",
            "29/29 [==============================] - 1s 10ms/step - loss: 32.3656 - val_loss: 25.2583\n",
            "24.463306427001953\n",
            "Epoch 2/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 22.2561 - val_loss: 15.4209\n",
            "15.777095794677734\n",
            "Epoch 3/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 13.4835 - val_loss: 9.7989\n",
            "10.550119400024414\n",
            "Epoch 4/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 9.0750 - val_loss: 7.8015\n",
            "8.813519477844238\n",
            "Epoch 5/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 6.6998 - val_loss: 6.7711\n",
            "7.670971393585205\n",
            "Epoch 6/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 5.8632 - val_loss: 6.1476\n",
            "7.0018134117126465\n",
            "Epoch 7/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 5.5938 - val_loss: 5.7428\n",
            "6.485861301422119\n",
            "Epoch 8/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 4.7070 - val_loss: 5.4657\n",
            "6.131750583648682\n",
            "Epoch 9/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 4.6577 - val_loss: 5.3263\n",
            "5.853699684143066\n",
            "Epoch 10/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.4975 - val_loss: 5.1774\n",
            "5.683608055114746\n",
            "Epoch 11/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.3673 - val_loss: 5.1212\n",
            "5.563756942749023\n",
            "Epoch 12/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.3846 - val_loss: 5.0556\n",
            "5.448127269744873\n",
            "Epoch 13/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.2013 - val_loss: 5.0082\n",
            "5.337088108062744\n",
            "Epoch 14/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.0480 - val_loss: 4.9451\n",
            "5.314207077026367\n",
            "Epoch 15/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 4.1977 - val_loss: 4.8729\n",
            "5.219822406768799\n",
            "Epoch 16/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.9900 - val_loss: 4.8225\n",
            "5.181288719177246\n",
            "Epoch 17/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.0304 - val_loss: 4.8919\n",
            "5.332242012023926\n",
            "Epoch 18/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.0552 - val_loss: 4.8023\n",
            "5.147295951843262\n",
            "Epoch 19/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7201 - val_loss: 4.7439\n",
            "5.070297718048096\n",
            "Epoch 20/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8141 - val_loss: 4.7930\n",
            "5.104888916015625\n",
            "Epoch 21/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8491 - val_loss: 4.7212\n",
            "5.157462120056152\n",
            "Epoch 22/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7561 - val_loss: 4.7519\n",
            "5.2404327392578125\n",
            "Epoch 23/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.6365 - val_loss: 4.7035\n",
            "5.06365966796875\n",
            "Epoch 24/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5273 - val_loss: 4.6679\n",
            "5.060451984405518\n",
            "Epoch 25/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5194 - val_loss: 4.6547\n",
            "5.118712425231934\n",
            "Epoch 26/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7002 - val_loss: 4.6475\n",
            "5.127058982849121\n",
            "Epoch 27/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4413 - val_loss: 4.6367\n",
            "5.095456123352051\n",
            "Epoch 28/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4750 - val_loss: 4.6255\n",
            "5.210581302642822\n",
            "Epoch 29/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5575 - val_loss: 4.6349\n",
            "5.27622652053833\n",
            "Epoch 30/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5975 - val_loss: 4.7505\n",
            "5.639411449432373\n",
            "Epoch 31/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.4442 - val_loss: 4.6162\n",
            "5.135238170623779\n",
            "Epoch 32/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5044 - val_loss: 4.5085\n",
            "5.240389823913574\n",
            "Epoch 33/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4716 - val_loss: 4.5881\n",
            "5.312783241271973\n",
            "Epoch 34/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3562 - val_loss: 4.5300\n",
            "5.305229663848877\n",
            "Epoch 35/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3686 - val_loss: 4.6027\n",
            "5.473867893218994\n",
            "Epoch 36/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4216 - val_loss: 4.6173\n",
            "5.587561130523682\n",
            "Epoch 37/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3555 - val_loss: 4.5733\n",
            "5.546865940093994\n",
            "Epoch 38/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.2139 - val_loss: 4.5377\n",
            "5.534581661224365\n",
            "Epoch 39/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2874 - val_loss: 4.5615\n",
            "5.61238431930542\n",
            "Epoch 40/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2647 - val_loss: 4.5264\n",
            "5.499013423919678\n",
            "Epoch 41/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.1714 - val_loss: 4.5254\n",
            "5.548120975494385\n",
            "Epoch 42/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.1307 - val_loss: 4.5007\n",
            "5.586676597595215\n",
            "Epoch 43/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0803 - val_loss: 4.5337\n",
            "5.669403076171875\n",
            "Epoch 44/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0002 - val_loss: 4.4796\n",
            "5.55614709854126\n",
            "Epoch 45/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9997 - val_loss: 4.4925\n",
            "5.73199462890625\n",
            "Epoch 46/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8979 - val_loss: 4.5003\n",
            "5.844686985015869\n",
            "Epoch 47/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0654 - val_loss: 4.5491\n",
            "5.794022560119629\n",
            "Epoch 48/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9541 - val_loss: 4.5349\n",
            "5.663647651672363\n",
            "Epoch 49/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.0048 - val_loss: 4.6098\n",
            "5.920407295227051\n",
            "Epoch 50/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8919 - val_loss: 4.6165\n",
            "5.909567356109619\n",
            "Epoch 51/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8783 - val_loss: 4.5400\n",
            "5.740633487701416\n",
            "Epoch 52/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7945 - val_loss: 4.6681\n",
            "5.941560745239258\n",
            "Epoch 53/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8785 - val_loss: 4.5328\n",
            "5.720941543579102\n",
            "Epoch 54/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6494 - val_loss: 4.5961\n",
            "5.829148292541504\n",
            "Epoch 55/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6894 - val_loss: 4.5420\n",
            "5.645512580871582\n",
            "Epoch 56/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6787 - val_loss: 4.6748\n",
            "5.787316799163818\n",
            "Epoch 57/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6662 - val_loss: 4.6224\n",
            "5.734279155731201\n",
            "Epoch 58/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6895 - val_loss: 4.6559\n",
            "5.63899564743042\n",
            "Epoch 59/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.5457 - val_loss: 4.7028\n",
            "5.857728958129883\n",
            "Epoch 60/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5865 - val_loss: 4.6633\n",
            "5.702855110168457\n",
            "Epoch 61/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5592 - val_loss: 4.6651\n",
            "5.653387069702148\n",
            "Epoch 62/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5648 - val_loss: 4.7029\n",
            "5.575590133666992\n",
            "Epoch 63/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5086 - val_loss: 4.7758\n",
            "5.638874530792236\n",
            "Epoch 64/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4939 - val_loss: 4.7929\n",
            "5.6381144523620605\n",
            "Epoch 65/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5409 - val_loss: 4.7656\n",
            "5.699917793273926\n",
            "Epoch 66/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4465 - val_loss: 4.7908\n",
            "5.58107328414917\n",
            "Epoch 67/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2866 - val_loss: 4.7677\n",
            "5.495148658752441\n",
            "Epoch 68/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3029 - val_loss: 4.7876\n",
            "5.500216960906982\n",
            "Epoch 69/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3986 - val_loss: 4.8496\n",
            "5.558627605438232\n",
            "Epoch 70/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2517 - val_loss: 4.9558\n",
            "5.751314163208008\n",
            "Epoch 71/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3361 - val_loss: 4.8022\n",
            "5.387548446655273\n",
            "Epoch 72/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3445 - val_loss: 4.8571\n",
            "5.529294967651367\n",
            "Epoch 73/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2484 - val_loss: 4.8384\n",
            "5.476687908172607\n",
            "Epoch 74/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2833 - val_loss: 4.9257\n",
            "5.576146602630615\n",
            "Epoch 75/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1994 - val_loss: 5.0903\n",
            "5.750773906707764\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "4.479568958282471\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "928\n",
            "Epoch 1/75\n",
            "29/29 [==============================] - 1s 12ms/step - loss: 27.2436 - val_loss: 13.3863\n",
            "14.626028060913086\n",
            "Epoch 2/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 12.1796 - val_loss: 9.2839\n",
            "11.077303886413574\n",
            "Epoch 3/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 8.8527 - val_loss: 7.3090\n",
            "9.288858413696289\n",
            "Epoch 4/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 6.5920 - val_loss: 6.2661\n",
            "8.331358909606934\n",
            "Epoch 5/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 6.2495 - val_loss: 5.6289\n",
            "7.920273780822754\n",
            "Epoch 6/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 5.3983 - val_loss: 5.3796\n",
            "7.837304592132568\n",
            "Epoch 7/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.8431 - val_loss: 5.0008\n",
            "7.025362491607666\n",
            "Epoch 8/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.7329 - val_loss: 4.8162\n",
            "7.14265251159668\n",
            "Epoch 9/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.5701 - val_loss: 4.7403\n",
            "7.098010540008545\n",
            "Epoch 10/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.4920 - val_loss: 4.7055\n",
            "7.215777397155762\n",
            "Epoch 11/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.3213 - val_loss: 4.5969\n",
            "6.916954517364502\n",
            "Epoch 12/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.1856 - val_loss: 4.5329\n",
            "6.665280818939209\n",
            "Epoch 13/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.3628 - val_loss: 4.4726\n",
            "6.823530197143555\n",
            "Epoch 14/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.0925 - val_loss: 4.4665\n",
            "7.081062316894531\n",
            "Epoch 15/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.1444 - val_loss: 4.3872\n",
            "6.653468608856201\n",
            "Epoch 16/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.9370 - val_loss: 4.4062\n",
            "6.557534694671631\n",
            "Epoch 17/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.0405 - val_loss: 4.3859\n",
            "6.719383716583252\n",
            "Epoch 18/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.9135 - val_loss: 4.4053\n",
            "7.198157787322998\n",
            "Epoch 19/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.8948 - val_loss: 4.4600\n",
            "6.503026008605957\n",
            "Epoch 20/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8577 - val_loss: 4.3498\n",
            "6.6786041259765625\n",
            "Epoch 21/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8095 - val_loss: 4.3643\n",
            "7.213249683380127\n",
            "Epoch 22/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.6335 - val_loss: 4.3689\n",
            "6.741589069366455\n",
            "Epoch 23/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.6941 - val_loss: 4.3062\n",
            "6.889832973480225\n",
            "Epoch 24/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5410 - val_loss: 4.3060\n",
            "6.900983810424805\n",
            "Epoch 25/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.6599 - val_loss: 4.2968\n",
            "7.182023048400879\n",
            "Epoch 26/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.6191 - val_loss: 4.2984\n",
            "7.2248454093933105\n",
            "Epoch 27/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4738 - val_loss: 4.4236\n",
            "8.100506782531738\n",
            "Epoch 28/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8740 - val_loss: 4.2999\n",
            "7.052080154418945\n",
            "Epoch 29/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3539 - val_loss: 4.3074\n",
            "7.874473571777344\n",
            "Epoch 30/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5061 - val_loss: 4.3351\n",
            "7.398182392120361\n",
            "Epoch 31/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5775 - val_loss: 4.2882\n",
            "7.613432884216309\n",
            "Epoch 32/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4268 - val_loss: 4.2885\n",
            "8.023543357849121\n",
            "Epoch 33/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3626 - val_loss: 4.3380\n",
            "7.6836113929748535\n",
            "Epoch 34/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3391 - val_loss: 4.3595\n",
            "8.842208862304688\n",
            "Epoch 35/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2574 - val_loss: 4.3228\n",
            "8.17548942565918\n",
            "Epoch 36/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2925 - val_loss: 4.2609\n",
            "8.603250503540039\n",
            "Epoch 37/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2995 - val_loss: 4.3106\n",
            "9.141753196716309\n",
            "Epoch 38/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2452 - val_loss: 4.3239\n",
            "8.90366268157959\n",
            "Epoch 39/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0964 - val_loss: 4.3091\n",
            "8.868708610534668\n",
            "Epoch 40/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0491 - val_loss: 4.3114\n",
            "9.186967849731445\n",
            "Epoch 41/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.1203 - val_loss: 4.4382\n",
            "9.022284507751465\n",
            "Epoch 42/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9116 - val_loss: 4.3609\n",
            "9.472043991088867\n",
            "Epoch 43/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0769 - val_loss: 4.4091\n",
            "9.233586311340332\n",
            "Epoch 44/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9599 - val_loss: 4.3780\n",
            "9.140625\n",
            "Epoch 45/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8944 - val_loss: 4.4128\n",
            "9.127157211303711\n",
            "Epoch 46/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9488 - val_loss: 4.4399\n",
            "9.815824508666992\n",
            "Epoch 47/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8562 - val_loss: 4.3886\n",
            "9.246142387390137\n",
            "Epoch 48/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8134 - val_loss: 4.5583\n",
            "10.418726921081543\n",
            "Epoch 49/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.0062 - val_loss: 4.5466\n",
            "9.192255020141602\n",
            "Epoch 50/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6960 - val_loss: 4.4772\n",
            "9.76553726196289\n",
            "Epoch 51/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.7888 - val_loss: 4.4805\n",
            "9.08066463470459\n",
            "Epoch 52/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6499 - val_loss: 4.5416\n",
            "9.416314125061035\n",
            "Epoch 53/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.6747 - val_loss: 4.6954\n",
            "9.91021728515625\n",
            "Epoch 54/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.7246 - val_loss: 4.6164\n",
            "8.703940391540527\n",
            "Epoch 55/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.5778 - val_loss: 4.6050\n",
            "9.381070137023926\n",
            "Epoch 56/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6223 - val_loss: 4.6022\n",
            "8.762469291687012\n",
            "Epoch 57/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5616 - val_loss: 4.6279\n",
            "8.61005687713623\n",
            "Epoch 58/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4696 - val_loss: 4.6334\n",
            "8.346867561340332\n",
            "Epoch 59/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4010 - val_loss: 4.6535\n",
            "8.629678726196289\n",
            "Epoch 60/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4463 - val_loss: 4.6811\n",
            "8.3281888961792\n",
            "Epoch 61/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2689 - val_loss: 4.7757\n",
            "7.785691261291504\n",
            "Epoch 62/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4848 - val_loss: 4.7161\n",
            "7.438271999359131\n",
            "Epoch 63/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4809 - val_loss: 4.7768\n",
            "7.631157398223877\n",
            "Epoch 64/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3503 - val_loss: 4.7988\n",
            "7.503215312957764\n",
            "Epoch 65/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2702 - val_loss: 4.7657\n",
            "7.418458938598633\n",
            "Epoch 66/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4001 - val_loss: 4.8407\n",
            "7.14307165145874\n",
            "Epoch 67/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2764 - val_loss: 4.8295\n",
            "6.830615043640137\n",
            "Epoch 68/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2501 - val_loss: 4.8363\n",
            "7.048041343688965\n",
            "Epoch 69/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2612 - val_loss: 4.9281\n",
            "6.884556770324707\n",
            "Epoch 70/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1948 - val_loss: 4.8832\n",
            "6.818406105041504\n",
            "Epoch 71/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1585 - val_loss: 4.9453\n",
            "6.633461952209473\n",
            "Epoch 72/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1337 - val_loss: 4.9989\n",
            "6.344375133514404\n",
            "Epoch 73/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1644 - val_loss: 5.0973\n",
            "6.7260308265686035\n",
            "Epoch 74/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.9787 - val_loss: 5.0571\n",
            "6.476325988769531\n",
            "Epoch 75/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.0911 - val_loss: 5.1287\n",
            "6.393266677856445\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "4.260873794555664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "928\n",
            "Epoch 1/75\n",
            "29/29 [==============================] - 1s 12ms/step - loss: 30.5492 - val_loss: 20.1481\n",
            "20.033071517944336\n",
            "Epoch 2/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 17.4308 - val_loss: 12.7316\n",
            "13.253449440002441\n",
            "Epoch 3/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 10.4400 - val_loss: 9.1809\n",
            "10.184042930603027\n",
            "Epoch 4/75\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 8.1043 - val_loss: 7.4408\n",
            "8.519972801208496\n",
            "Epoch 5/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 6.5956 - val_loss: 6.4968\n",
            "7.554340839385986\n",
            "Epoch 6/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 5.6000 - val_loss: 5.9760\n",
            "6.872422218322754\n",
            "Epoch 7/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 5.1276 - val_loss: 5.7081\n",
            "6.399218559265137\n",
            "Epoch 8/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 4.6055 - val_loss: 5.7582\n",
            "6.46119499206543\n",
            "Epoch 9/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.5447 - val_loss: 5.5033\n",
            "6.023379325866699\n",
            "Epoch 10/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.3269 - val_loss: 5.4226\n",
            "5.755646705627441\n",
            "Epoch 11/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.2735 - val_loss: 5.3669\n",
            "5.558191776275635\n",
            "Epoch 12/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.8493 - val_loss: 5.3153\n",
            "5.4955644607543945\n",
            "Epoch 13/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.8517 - val_loss: 5.2650\n",
            "5.480750560760498\n",
            "Epoch 14/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8705 - val_loss: 5.2350\n",
            "5.5151214599609375\n",
            "Epoch 15/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.9284 - val_loss: 5.2376\n",
            "5.458481788635254\n",
            "Epoch 16/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8857 - val_loss: 5.2413\n",
            "5.50351619720459\n",
            "Epoch 17/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.6781 - val_loss: 5.2178\n",
            "5.417839050292969\n",
            "Epoch 18/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7792 - val_loss: 5.2033\n",
            "5.42936372756958\n",
            "Epoch 19/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5952 - val_loss: 5.1331\n",
            "5.384228706359863\n",
            "Epoch 20/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.5669 - val_loss: 5.1827\n",
            "5.55733060836792\n",
            "Epoch 21/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7991 - val_loss: 5.1783\n",
            "5.612823486328125\n",
            "Epoch 22/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5212 - val_loss: 5.1334\n",
            "5.5577545166015625\n",
            "Epoch 23/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.3970 - val_loss: 5.2454\n",
            "5.895074844360352\n",
            "Epoch 24/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.3828 - val_loss: 5.0986\n",
            "5.784417152404785\n",
            "Epoch 25/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.5006 - val_loss: 5.1556\n",
            "5.80635404586792\n",
            "Epoch 26/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4577 - val_loss: 5.2007\n",
            "6.159263610839844\n",
            "Epoch 27/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2858 - val_loss: 5.1853\n",
            "6.208528995513916\n",
            "Epoch 28/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4051 - val_loss: 5.1348\n",
            "6.229678630828857\n",
            "Epoch 29/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.4444 - val_loss: 5.1592\n",
            "6.48839807510376\n",
            "Epoch 30/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2854 - val_loss: 5.2291\n",
            "6.746539115905762\n",
            "Epoch 31/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2659 - val_loss: 5.2182\n",
            "6.769952297210693\n",
            "Epoch 32/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2725 - val_loss: 5.1963\n",
            "6.914146900177002\n",
            "Epoch 33/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2424 - val_loss: 5.2159\n",
            "7.084156513214111\n",
            "Epoch 34/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2912 - val_loss: 5.1585\n",
            "7.018802165985107\n",
            "Epoch 35/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2296 - val_loss: 5.1906\n",
            "7.331855773925781\n",
            "Epoch 36/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9623 - val_loss: 5.2708\n",
            "7.716703414916992\n",
            "Epoch 37/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.1673 - val_loss: 5.2294\n",
            "8.034842491149902\n",
            "Epoch 38/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0643 - val_loss: 5.2805\n",
            "8.504412651062012\n",
            "Epoch 39/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0659 - val_loss: 5.3301\n",
            "8.591862678527832\n",
            "Epoch 40/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8206 - val_loss: 5.2497\n",
            "8.664687156677246\n",
            "Epoch 41/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0089 - val_loss: 5.3349\n",
            "8.875420570373535\n",
            "Epoch 42/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8026 - val_loss: 5.2919\n",
            "8.870194435119629\n",
            "Epoch 43/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9438 - val_loss: 5.2788\n",
            "9.213679313659668\n",
            "Epoch 44/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8398 - val_loss: 5.3703\n",
            "9.758438110351562\n",
            "Epoch 45/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8322 - val_loss: 5.3168\n",
            "9.545327186584473\n",
            "Epoch 46/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.9164 - val_loss: 5.4427\n",
            "9.701417922973633\n",
            "Epoch 47/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8221 - val_loss: 5.3990\n",
            "9.657671928405762\n",
            "Epoch 48/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7651 - val_loss: 5.4400\n",
            "9.929281234741211\n",
            "Epoch 49/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7977 - val_loss: 5.4282\n",
            "9.901854515075684\n",
            "Epoch 50/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7438 - val_loss: 5.3512\n",
            "9.287447929382324\n",
            "Epoch 51/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6615 - val_loss: 5.4227\n",
            "9.606549263000488\n",
            "Epoch 52/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7844 - val_loss: 5.4343\n",
            "9.445076942443848\n",
            "Epoch 53/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6230 - val_loss: 5.7814\n",
            "10.03576374053955\n",
            "Epoch 54/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7152 - val_loss: 5.6041\n",
            "9.556246757507324\n",
            "Epoch 55/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5280 - val_loss: 5.5467\n",
            "9.207507133483887\n",
            "Epoch 56/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5858 - val_loss: 5.5588\n",
            "8.820131301879883\n",
            "Epoch 57/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6244 - val_loss: 5.6071\n",
            "9.054824829101562\n",
            "Epoch 58/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4608 - val_loss: 5.5983\n",
            "8.91072940826416\n",
            "Epoch 59/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6497 - val_loss: 5.7375\n",
            "9.45884895324707\n",
            "Epoch 60/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4272 - val_loss: 5.9026\n",
            "8.835039138793945\n",
            "Epoch 61/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6045 - val_loss: 5.6703\n",
            "8.288020133972168\n",
            "Epoch 62/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5004 - val_loss: 5.7429\n",
            "8.381999969482422\n",
            "Epoch 63/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4798 - val_loss: 5.8953\n",
            "7.8134894371032715\n",
            "Epoch 64/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4355 - val_loss: 5.6836\n",
            "7.387167930603027\n",
            "Epoch 65/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3429 - val_loss: 5.8975\n",
            "7.52656364440918\n",
            "Epoch 66/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3663 - val_loss: 6.0749\n",
            "7.701596260070801\n",
            "Epoch 67/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2828 - val_loss: 5.8178\n",
            "7.32675838470459\n",
            "Epoch 68/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3779 - val_loss: 5.9095\n",
            "7.091181755065918\n",
            "Epoch 69/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1912 - val_loss: 5.9055\n",
            "6.7053303718566895\n",
            "Epoch 70/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1532 - val_loss: 5.9768\n",
            "6.938653945922852\n",
            "Epoch 71/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2433 - val_loss: 5.9947\n",
            "6.696564674377441\n",
            "Epoch 72/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1090 - val_loss: 5.9996\n",
            "6.794166564941406\n",
            "Epoch 73/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.1540 - val_loss: 6.0395\n",
            "6.611433982849121\n",
            "Epoch 74/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.1395 - val_loss: 6.1558\n",
            "6.4148268699646\n",
            "Epoch 75/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1858 - val_loss: 6.0313\n",
            "6.231512546539307\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "5.098584175109863\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "928\n",
            "Epoch 1/75\n",
            "29/29 [==============================] - 1s 11ms/step - loss: 28.8773 - val_loss: 16.0339\n",
            "16.02012825012207\n",
            "Epoch 2/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 13.3582 - val_loss: 10.8280\n",
            "12.155990600585938\n",
            "Epoch 3/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 9.7127 - val_loss: 8.2331\n",
            "9.898890495300293\n",
            "Epoch 4/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 7.8834 - val_loss: 6.8361\n",
            "8.718558311462402\n",
            "Epoch 5/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 6.5020 - val_loss: 6.0534\n",
            "8.082218170166016\n",
            "Epoch 6/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 5.2412 - val_loss: 5.6254\n",
            "7.9637956619262695\n",
            "Epoch 7/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 5.1507 - val_loss: 5.2300\n",
            "7.446378707885742\n",
            "Epoch 8/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 4.8128 - val_loss: 5.0467\n",
            "7.334765434265137\n",
            "Epoch 9/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.4746 - val_loss: 4.8593\n",
            "7.122476100921631\n",
            "Epoch 10/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.2656 - val_loss: 4.8285\n",
            "7.268328666687012\n",
            "Epoch 11/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.2568 - val_loss: 4.7375\n",
            "7.168028831481934\n",
            "Epoch 12/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.0215 - val_loss: 4.6412\n",
            "6.759444236755371\n",
            "Epoch 13/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8739 - val_loss: 4.6073\n",
            "6.609548568725586\n",
            "Epoch 14/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 4.2761 - val_loss: 4.5738\n",
            "6.633337497711182\n",
            "Epoch 15/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8016 - val_loss: 4.6325\n",
            "7.014847278594971\n",
            "Epoch 16/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.9372 - val_loss: 4.5867\n",
            "6.966248512268066\n",
            "Epoch 17/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.8504 - val_loss: 4.5163\n",
            "6.7384114265441895\n",
            "Epoch 18/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.7788 - val_loss: 4.5419\n",
            "6.745244979858398\n",
            "Epoch 19/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.7401 - val_loss: 4.4954\n",
            "6.870955944061279\n",
            "Epoch 20/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5491 - val_loss: 4.5490\n",
            "7.4276814460754395\n",
            "Epoch 21/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.8099 - val_loss: 4.6253\n",
            "7.479624271392822\n",
            "Epoch 22/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4952 - val_loss: 4.5642\n",
            "7.626633644104004\n",
            "Epoch 23/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5934 - val_loss: 4.5621\n",
            "7.9649271965026855\n",
            "Epoch 24/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5578 - val_loss: 4.5214\n",
            "7.865394592285156\n",
            "Epoch 25/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.5123 - val_loss: 4.5377\n",
            "7.994713306427002\n",
            "Epoch 26/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.6111 - val_loss: 4.5962\n",
            "8.56123161315918\n",
            "Epoch 27/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.5255 - val_loss: 4.5661\n",
            "8.762925148010254\n",
            "Epoch 28/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3865 - val_loss: 4.6147\n",
            "9.076570510864258\n",
            "Epoch 29/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.4586 - val_loss: 4.5886\n",
            "9.291678428649902\n",
            "Epoch 30/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.3865 - val_loss: 4.6211\n",
            "10.126282691955566\n",
            "Epoch 31/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2462 - val_loss: 4.5723\n",
            "9.989625930786133\n",
            "Epoch 32/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 3.2743 - val_loss: 4.6192\n",
            "11.067265510559082\n",
            "Epoch 33/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.1773 - val_loss: 4.6154\n",
            "11.511524200439453\n",
            "Epoch 34/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.1117 - val_loss: 4.6075\n",
            "12.360739707946777\n",
            "Epoch 35/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.1388 - val_loss: 4.6455\n",
            "12.381299018859863\n",
            "Epoch 36/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.1181 - val_loss: 4.6736\n",
            "13.332015037536621\n",
            "Epoch 37/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0926 - val_loss: 4.6567\n",
            "14.320799827575684\n",
            "Epoch 38/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9500 - val_loss: 4.6672\n",
            "14.2617769241333\n",
            "Epoch 39/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.0311 - val_loss: 4.6664\n",
            "14.38202953338623\n",
            "Epoch 40/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9412 - val_loss: 4.7242\n",
            "14.616639137268066\n",
            "Epoch 41/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9180 - val_loss: 4.7563\n",
            "14.921107292175293\n",
            "Epoch 42/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.8905 - val_loss: 4.8175\n",
            "15.878637313842773\n",
            "Epoch 43/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.9599 - val_loss: 4.8186\n",
            "16.626220703125\n",
            "Epoch 44/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.8557 - val_loss: 4.8209\n",
            "15.95999526977539\n",
            "Epoch 45/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.8411 - val_loss: 4.8458\n",
            "16.343515396118164\n",
            "Epoch 46/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7361 - val_loss: 4.8029\n",
            "15.983922958374023\n",
            "Epoch 47/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6945 - val_loss: 4.9673\n",
            "16.395822525024414\n",
            "Epoch 48/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7618 - val_loss: 4.8942\n",
            "16.391151428222656\n",
            "Epoch 49/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7028 - val_loss: 4.8894\n",
            "15.50843620300293\n",
            "Epoch 50/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.6730 - val_loss: 4.9249\n",
            "16.078462600708008\n",
            "Epoch 51/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.7132 - val_loss: 4.9507\n",
            "16.897783279418945\n",
            "Epoch 52/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5602 - val_loss: 5.0289\n",
            "16.881235122680664\n",
            "Epoch 53/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5373 - val_loss: 4.9190\n",
            "16.572315216064453\n",
            "Epoch 54/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4038 - val_loss: 5.0242\n",
            "15.974699974060059\n",
            "Epoch 55/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5455 - val_loss: 5.0282\n",
            "16.50053596496582\n",
            "Epoch 56/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3467 - val_loss: 5.0093\n",
            "16.13311004638672\n",
            "Epoch 57/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4543 - val_loss: 5.0400\n",
            "16.099470138549805\n",
            "Epoch 58/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4599 - val_loss: 5.0942\n",
            "15.656758308410645\n",
            "Epoch 59/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.4294 - val_loss: 5.0851\n",
            "15.335457801818848\n",
            "Epoch 60/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.4177 - val_loss: 5.1044\n",
            "16.782148361206055\n",
            "Epoch 61/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3585 - val_loss: 5.3117\n",
            "16.76308822631836\n",
            "Epoch 62/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3784 - val_loss: 5.2320\n",
            "16.066707611083984\n",
            "Epoch 63/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.2700 - val_loss: 5.2588\n",
            "15.04000186920166\n",
            "Epoch 64/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2458 - val_loss: 5.2100\n",
            "15.390042304992676\n",
            "Epoch 65/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.1138 - val_loss: 5.3109\n",
            "15.20814037322998\n",
            "Epoch 66/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.2262 - val_loss: 5.2931\n",
            "14.752120018005371\n",
            "Epoch 67/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.2101 - val_loss: 5.2412\n",
            "14.74435043334961\n",
            "Epoch 68/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1651 - val_loss: 5.3959\n",
            "14.946550369262695\n",
            "Epoch 69/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1940 - val_loss: 5.3904\n",
            "15.050803184509277\n",
            "Epoch 70/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1797 - val_loss: 5.3456\n",
            "14.346652030944824\n",
            "Epoch 71/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.0284 - val_loss: 5.4025\n",
            "14.600643157958984\n",
            "Epoch 72/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.0627 - val_loss: 5.3600\n",
            "14.087008476257324\n",
            "Epoch 73/75\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.9926 - val_loss: 5.4660\n",
            "13.060867309570312\n",
            "Epoch 74/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.0380 - val_loss: 5.4861\n",
            "13.160638809204102\n",
            "Epoch 75/75\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.9927 - val_loss: 5.5223\n",
            "12.174646377563477\n",
            "------------------------------------------------------------------------\n",
            "Minimal loss epoch and value\n",
            "4.4953999519348145\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 4.907576084136963\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 5.090259552001953\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 5.128742694854736\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 6.031323432922363\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 5.522299289703369\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Loss: 5.336040210723877\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APKxKpkTEL_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43346831-afff-47ea-a6b1-55b4774f74fa"
      },
      "source": [
        "print(minimal_val_losses_index)\n",
        "print(minimal_val_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19, 43, 35, 23, 18]\n",
            "[4.0626349449157715, 4.479568958282471, 4.260873794555664, 5.098584175109863, 4.4953999519348145]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Ksdpn1HgGSaU",
        "outputId": "5ab17e6b-73b4-4dc7-ef4a-79c35a613d65"
      },
      "source": [
        "def draw_curves(history, key1='accuracy', ylim1=(0.8, 1.00), \n",
        "                key2='loss', ylim2=(0.0, 1.0)):\n",
        "    plt.figure(figsize=(12,4))\n",
        " \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history[key2], \"r--\")\n",
        "    plt.plot(history.history['val_' + key2], \"g--\")\n",
        "    plt.ylabel(key2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylim(ylim2)\n",
        "    plt.legend(['train', 'test'], loc='best')\n",
        "     \n",
        "    plt.show()\n",
        "     \n",
        "draw_curves(history, key1='accuracy', ylim1=(0.7, 0.95), \n",
        "            key2='loss', ylim2=(0.0, 0.2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}