{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVEX5B5IjnqJ"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9YJSmUVR_Tf"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R23Bet1Qmf8S"
      },
      "source": [
        "files = []\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s01.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s02.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s03.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s04.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s05.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s06.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s07.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s08.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s09.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s10.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s11.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s12.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s13.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s14.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s15.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s16.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s17.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s18.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s19.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s20.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s21.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s22.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s23.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s24.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s25.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s26.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s27.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s28.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s29.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s30.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s31.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s32.dat')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3CmentroEDU"
      },
      "source": [
        "# wczytanie danych i preprocessing 1\n",
        "table_of_dataX = []\n",
        "table_of_dataY = []\n",
        "for i in range(32):\n",
        "  f = files[i]\n",
        "  with open(f, 'rb') as f: content = pickle.load(f, encoding='latin1')\n",
        "  data = content['data']\n",
        "  labels = content['labels']\n",
        "  for j in range(40): #40 filmow\n",
        "    vector_input = []\n",
        "    tmpX = data[j]\n",
        "    tmpY = labels[j]\n",
        "    table_of_dataY.append(tmpY)\n",
        "    for k in range(40): #40 sygnalow\n",
        "      mean = np.mean(tmpX[k])\n",
        "      median = np.median(tmpX[k])\n",
        "      max = np.max(tmpX[k])\n",
        "      min = np.min(tmpX[k])\n",
        "      variance = np.var(tmpX[k])\n",
        "      skewness = skew(tmpX[k])\n",
        "      kur = kurtosis(tmpX[k])\n",
        "      vector_input.append(mean)\n",
        "      vector_input.append(median)\n",
        "      vector_input.append(max)\n",
        "      vector_input.append(min)\n",
        "      vector_input.append(variance)\n",
        "      vector_input.append(skewness)\n",
        "      vector_input.append(kur)\n",
        "    table_of_dataX.append(vector_input)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4I9C_wPtTyj"
      },
      "source": [
        "table_of_dataX = np.asarray(table_of_dataX)\n",
        "table_of_dataY = np.asarray(table_of_dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHzPV3vpnlSe"
      },
      "source": [
        "randnums = [   4,    5,   26,   34,   36,   52,   56,   57,   72,   80,   87,\n",
        "         92,  119,  134,  141,  155,  165,  197,  228,  233,  239,  289,\n",
        "        291,  293,  311,  315,  318,  334,  371,  375,  383,  416,  434,\n",
        "        435,  442,  444,  450,  471,  472,  475,  502,  504,  509,  516,\n",
        "        540,  549,  550,  555,  587,  603,  622,  623,  635,  648,  676,\n",
        "        692,  693,  694,  702,  716,  732,  745,  754,  758,  760,  764,\n",
        "        772,  773,  778,  779,  787,  814,  815,  817,  846,  850,  865,\n",
        "        872,  876,  881,  889,  895,  901,  935,  936,  963,  976,  984,\n",
        "        991,  992,  996, 1010, 1013, 1017, 1020, 1060, 1067, 1070, 1071,\n",
        "       1087, 1088, 1091, 1098, 1118, 1122, 1130, 1148, 1150, 1166, 1171,\n",
        "       1187, 1197, 1213, 1219, 1225, 1240, 1259, 1260, 1264, 1271]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7lWZYOWLXC_"
      },
      "source": [
        "testdataX = []\n",
        "testdataY = []\n",
        "for i in randnums:\n",
        "  testdataX.append(table_of_dataX[i])\n",
        "  testdataY.append(table_of_dataY[i])\n",
        "table_of_dataX_learn = np.delete(table_of_dataX, randnums, 0)\n",
        "table_of_dataY_learn = np.delete(table_of_dataY, randnums, 0)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASPEqxuzrohL"
      },
      "source": [
        "divider_array = np.max(np.abs(table_of_dataX_learn), axis=0)\n",
        "table_of_dataX_learn_normed = table_of_dataX_learn/divider_array\n",
        "testdataX_normed = testdataX/divider_array\n",
        "testdataY = np.asarray(testdataY)\n",
        "len(table_of_dataX_learn_normed)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxukLCdBFaw4"
      },
      "source": [
        "\n",
        "class MyCustomCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        res_eval_1 = self.model.evaluate(testdataX_normed, testdataY, verbose = 0)\n",
        "        print(res_eval_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2yLbQoUSCwj"
      },
      "source": [
        "num_folds = 5\n",
        "fold_no = 1\n",
        "loss_per_fold = []\n",
        "minimal_val_losses = []\n",
        "minimal_val_losses_index = []\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "for train, test in kfold.split(table_of_dataX_learn_normed, table_of_dataY_learn):\n",
        "  model = Sequential()\n",
        "  model.add(layers.Dense(70, input_shape=(280,), activation='relu'))\n",
        "  model.add(layers.Dense(25,activation='relu',))\n",
        "  model.add(layers.Dense(4))\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='mse'\n",
        "              )\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  print(len(table_of_dataX_learn_normed[train]))\n",
        "  #callback = keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
        "  myval_callback = MyCustomCallback()\n",
        "  history = model.fit(table_of_dataX_learn_normed[train], \n",
        "                    table_of_dataY_learn[train], \n",
        "                    epochs=75,\n",
        "                    verbose=1,\n",
        "                    batch_size = 32,\n",
        "                    callbacks = [myval_callback],\n",
        "                    validation_data = (table_of_dataX_learn_normed[test], table_of_dataY_learn[test])\n",
        "                   )\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Minimal loss epoch and value')\n",
        "  min_loss = np.min(history.history['val_loss'])\n",
        "  minimal_val_losses.append(min_loss)\n",
        "  minimal_val_losses_index.append(np.argmin(history.history['val_loss']))\n",
        "  print(min_loss) \n",
        "  scores = model.evaluate(table_of_dataX_learn_normed[test], table_of_dataY_learn[test], verbose=0)\n",
        "  loss_per_fold.append(scores)\n",
        "  fold_no = fold_no + 1\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(loss_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]}')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APKxKpkTEL_S"
      },
      "source": [
        "print(minimal_val_losses_index)\n",
        "print(minimal_val_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksdpn1HgGSaU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}