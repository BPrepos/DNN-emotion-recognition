{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6by9Hq0pz9AM"
      },
      "source": [
        "from numpy import loadtxt, array\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9YJSmUVR_Tf"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R23Bet1Qmf8S"
      },
      "source": [
        "files = []\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s01.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s02.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s03.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s04.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s05.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s06.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s07.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s08.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s09.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s10.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s11.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s12.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s13.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s14.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s15.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s16.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s17.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s18.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s19.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s20.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s21.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s22.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s23.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s24.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s25.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s26.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s27.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s28.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s29.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s30.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s31.dat')\n",
        "files.append('/content/gdrive/MyDrive/deapdata/data_preprocessed_python/s32.dat')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3CmentroEDU"
      },
      "source": [
        "# wczytanie danych i preprocessing 2\n",
        "table_of_dataX = []\n",
        "table_of_dataY = []\n",
        "for i in range(32):\n",
        "  f = files[i]\n",
        "  with open(f, 'rb') as f: content = pickle.load(f, encoding='latin1')\n",
        "  data = content['data']\n",
        "  labels = content['labels']\n",
        "  for j in range(40): #40 filmow\n",
        "    tmpX = data[j]\n",
        "    tmpY = labels[j]\n",
        "    for l in range(7):\n",
        "      table_of_dataY.append(tmpY)\n",
        "      signal_number = l*1000 + 1000\n",
        "      vector_input = []\n",
        "      for k in range(40):\n",
        "        mean = np.mean(tmpX[k][signal_number:signal_number+1000])\n",
        "        median = np.median(tmpX[k][signal_number:signal_number+1000])\n",
        "        max = np.max(tmpX[k][signal_number:signal_number+1000])\n",
        "        min = np.min(tmpX[k][signal_number:signal_number+1000])\n",
        "        variance = np.var(tmpX[k][signal_number:signal_number+1000])\n",
        "        skewness = skew(tmpX[k][signal_number:signal_number+1000])\n",
        "        kur = kurtosis(tmpX[k][signal_number:signal_number+1000])\n",
        "        vector_input.append(mean)\n",
        "        vector_input.append(median)\n",
        "        vector_input.append(max)\n",
        "        vector_input.append(min)\n",
        "        vector_input.append(variance)\n",
        "        vector_input.append(skewness)\n",
        "        vector_input.append(kur)\n",
        "      table_of_dataX.append(vector_input)\n",
        "\n",
        "\n",
        "        \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4I9C_wPtTyj"
      },
      "source": [
        "table_of_dataX = np.asarray(table_of_dataX)\n",
        "table_of_dataY = np.asarray(table_of_dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlBZVZigH0eu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHzPV3vpnlSe"
      },
      "source": [
        "print(len(table_of_dataX))\n",
        "print(len(table_of_dataY))\n",
        "randnums = [1384, 5436, 5904, 6490,   76, 2066, 5068, 6085, 4360, 2995, 4069,\n",
        "       8334, 6052, 6078, 1161, 1434,  123, 5356, 6270, 3977, 1882, 2556,\n",
        "       6286, 5572, 6719, 2112, 5237, 4023, 1925, 3396, 5688, 2261, 1458,\n",
        "       3869, 7851, 6049, 2790, 6500, 3084, 4834, 8263,  774, 6964, 3561,\n",
        "        254, 2713, 6468, 8799, 2394, 6291, 5306, 5358, 7856, 3734,  928,\n",
        "       6360, 7380, 8526,   54, 7409, 2318, 4415, 5057, 4387, 8366, 2255,\n",
        "         16, 3335, 7950, 5598, 8599, 5785,  342, 8214, 8417, 6012, 2856,\n",
        "       6481, 2403, 7391, 4168, 7017, 2347,  525, 6317, 7094, 3157, 6739,\n",
        "       8934, 4432, 6540, 6918,  655, 6005, 5727, 8691, 7599, 4671, 3492,\n",
        "       2632, 3687, 2327, 4030, 2580, 4781, 7005, 5494,  668, 4913,  919,\n",
        "       2385, 6939, 1746, 4729, 6752, 1895, 5131, 5024, 3809, 6461, 1976,\n",
        "       8156, 3463, 3271, 5393, 7203, 8580,  863, 1632, 3217, 2501, 2633,\n",
        "        691, 3954,  647, 7216, 4252, 2172, 3248, 5536, 8937, 2378, 1874,\n",
        "       3046, 2573, 5266, 3426,  992, 2147, 2314, 4304, 5168, 1611, 3998,\n",
        "       4018, 4580, 2538, 5037, 1915, 5414, 1708, 4221, 7267,  628, 1771,\n",
        "        487, 3290,   75, 1673, 2636, 7300,  169, 4796, 6829, 6357, 5034,\n",
        "       6025,  930, 7758, 6761, 3302, 2564, 2915,  625,   28, 5147, 4678,\n",
        "       4849,  384, 6197, 5159, 6491, 4155, 7387, 7742,  190,  155, 7302,\n",
        "       5490, 8688, 2226, 4378, 7898,  255, 8493, 6002, 2640, 1132, 4298,\n",
        "       3397, 2105,  507,  824, 5343, 7288, 8573,  425, 4885, 7767, 3446,\n",
        "       4709, 4504, 2192, 3962, 3526, 1705, 7159, 5132, 5510, 5791,  423,\n",
        "       1164, 5879,   97, 8729, 4535, 4703, 2051, 2769, 8062,  117, 1340,\n",
        "       3948, 4784,  269, 3909, 5736, 5613, 5756, 5950, 6749, 7268, 8893,\n",
        "       7848, 8152, 4951, 5959, 7385, 5245, 2510, 7979, 5404, 2033, 8460,\n",
        "       8666, 3731, 2745, 3647, 3287,  113, 8553, 8343,   30, 7762, 6321,\n",
        "       8083, 6894, 2832, 6495, 4364, 3807,  516, 3618, 5115, 8483, 6073,\n",
        "       7867,   20, 3692, 2968, 7920, 7708, 7405, 8888, 3898, 6108, 1468,\n",
        "       6043, 8679, 5182, 2127, 6203, 8102,  552, 2067, 4398, 6842, 2526,\n",
        "       5183, 3292, 7390, 5628, 8139, 3187, 7766, 2016, 2402, 2469,  226,\n",
        "       4965, 6559, 8420, 6863, 5728, 3466, 2541, 3899,  358, 3961,  331,\n",
        "       6574, 4016, 7083, 1082, 2550, 7228, 3027, 1263, 3790, 8283, 6834,\n",
        "       7981, 2753, 3310, 6457, 4776, 5575, 4076, 4568, 2487, 7059, 7065,\n",
        "       3014, 5504, 7813, 4952, 3401, 3728, 5009, 2803, 2094, 7080, 4979,\n",
        "       8584, 2211, 2405, 8665, 2119, 1916, 4928, 5742, 4802, 1239, 6482,\n",
        "       4503,  445, 5841,  979, 6783, 7791, 8316, 2032, 5953, 6621, 6543,\n",
        "       2741, 8349, 1586, 1709, 8434, 3861, 5425, 3078, 7902,  150, 6312,\n",
        "       3529, 6100, 4311, 1012,  114, 5275,  709,  237, 7585, 6423, 2730,\n",
        "       8110, 1805, 8262, 1292,  230, 8026, 5178, 2729, 1947, 5829, 2213,\n",
        "       5107, 8547, 2162, 6873, 3645, 3806, 4987, 7883, 3327, 2092, 8113,\n",
        "        970, 7675, 4444, 6023, 8383, 3877,  260, 1145,  197, 7580, 8958,\n",
        "       8217, 1844, 5270, 7538, 1537, 3438, 1668, 1546, 1053, 8127, 5284,\n",
        "       6265, 4607, 3969, 5189, 5983, 3770, 1619, 1604, 7489, 3578, 3069,\n",
        "       1652, 3648, 7117,  145, 8855, 8823, 5715, 7231, 1583, 2534, 2604,\n",
        "       5065, 1525, 4534, 5172, 2698, 8690, 3258,  450, 3740, 1186, 8296,\n",
        "       8467, 3193, 4902, 6903, 7384, 3574, 8662, 1970, 5503,  752, 6145,\n",
        "       6388, 5537, 8211, 3348, 2999, 8908, 6283, 6187, 5901, 2905, 3135,\n",
        "       6213, 4775, 3030, 8187,  545, 8765, 5695, 8453, 1811, 5552, 2278,\n",
        "       1268,  821, 3864, 3002,  903, 2784, 5400, 4279, 7386, 7726, 6768,\n",
        "       4828, 4110, 4427, 3478, 3400, 1077, 5970, 5941, 7421, 8535, 4512,\n",
        "       1351, 2475, 4414, 5875,  246, 7165, 6603, 6170, 2717, 8704, 8112,\n",
        "       3718, 2449,  491, 5502, 3927, 4598,  400, 7531, 8025, 6544, 1756,\n",
        "       2584, 1435, 3839, 7221, 7839, 3841,  889, 2476, 7976, 4942, 7816,\n",
        "       7881, 8288, 1450, 3391, 8633, 5748, 4050, 4319, 4777, 5474, 5489,\n",
        "       8606, 1023, 2069, 2782, 5822, 5421,  806, 2085, 2083, 3374, 2149,\n",
        "       5586, 3614, 4954, 7025, 4048, 7567, 1316, 4012, 4895, 8093, 7395,\n",
        "       2840, 8181, 6199, 7455, 7927, 6076, 1501, 8046,  303, 4997, 3065,\n",
        "       1629, 7800, 8131, 4674, 7621,  939, 1591, 5579,  498,  119, 4290,\n",
        "       5328, 6191, 2269, 2533,  317,  773, 1802, 7733, 8274, 5040, 1189,\n",
        "        472, 8596, 5923, 4661, 4718, 1955, 5863, 7542, 3564, 8374, 7480,\n",
        "       5750, 4247, 5680,  921,  382, 1639, 1710, 4604,  393, 5694, 1312,\n",
        "       5063, 3500, 7179, 3493, 7081, 4797, 7510,  908, 6212, 3088, 1330,\n",
        "       6536, 1819, 4371, 7048, 4814, 2177, 1456, 3933, 4227, 1803, 8386,\n",
        "       8240, 4163,  918, 4267, 1007, 1769, 7549, 1313, 2462, 7962, 8155,\n",
        "       1893, 5173, 4600, 7878, 7256, 6888, 8272, 2976, 7609, 2148, 4741,\n",
        "       6344, 5096, 1631, 5216, 8647, 8699, 7258, 1890, 4217, 8475, 4348,\n",
        "       7553,   59, 2684,  288, 2257, 7995, 7183, 8505, 7474, 7400, 2049,\n",
        "       1020, 8175, 5327, 7397,  298,  378,  129, 1355, 1550,  394,  690,\n",
        "       1834, 6047, 2110,  902, 7134, 2718, 3086, 7124, 2972,  883,  714,\n",
        "       7148, 5280, 4548, 2540, 4379, 5348, 7581, 6913, 6000,  994, 5413,\n",
        "       5582, 7956, 8946, 4625, 3129,  601, 2099, 7020, 3214, 2188, 4169,\n",
        "       1360, 8718, 1353, 4164, 4602, 7652, 1033,  229, 2562, 2971, 7121,\n",
        "       2965, 3782, 6833, 3896, 7445, 5346, 1256, 1529, 1179, 3314, 5021,\n",
        "        131, 8295, 1255, 8901, 4208, 6109, 7135, 5305, 8027, 2876, 6184,\n",
        "       4394, 7332, 5522, 4582, 6667, 3060, 1930, 2699,  105, 5538, 1982,\n",
        "       1460,  837, 5482, 8836, 7342, 6036,  572,  999, 6809,  193, 4140,\n",
        "       8607, 6275,  372, 6989,  730, 7360, 1038, 2158, 6685, 6643, 5082,\n",
        "       8670, 6831, 8401, 2543, 3498, 6725, 3608, 7415, 5925, 6451, 3481,\n",
        "       5250, 3383, 5767,  208, 5796, 3266, 2825, 3720,  380, 3262, 3679,\n",
        "       6564, 1412, 5997, 6753, 8425, 4565, 4791, 3652,  589, 1806, 4291,\n",
        "       4060, 7871, 4649, 1610, 6923, 6675, 7032, 4748,  688, 4795,  236,\n",
        "       4136, 8369, 2060, 3646, 6008, 7077, 6048, 1199, 5787, 6206, 8237,\n",
        "       6554, 8733, 4266, 5815, 1119]\n",
        "testdataX = []\n",
        "testdataY = []\n",
        "for i in randnums:\n",
        "  testdataX.append(table_of_dataX[i])\n",
        "  testdataY.append(table_of_dataY[i])\n",
        "table_of_dataX_learn = np.delete(table_of_dataX, randnums, 0)\n",
        "table_of_dataY_learn = np.delete(table_of_dataY, randnums, 0)\n",
        "print(len(table_of_dataX_learn))\n",
        "print(len(table_of_dataY_learn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASPEqxuzrohL"
      },
      "source": [
        "divider_array = np.max(np.abs(table_of_dataX_learn), axis=0)\n",
        "table_of_dataX_learn_normed = table_of_dataX_learn/divider_array\n",
        "testdataX_normed = testdataX/divider_array\n",
        "testdataY = np.asarray(testdataY)\n",
        "len(table_of_dataX_learn_normed)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxukLCdBFaw4"
      },
      "source": [
        "\n",
        "class MyCustomCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        res_eval_1 = self.model.evaluate(testdataX_normed, testdataY, verbose = 0)\n",
        "        print(res_eval_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEYFxnk1OJBR"
      },
      "source": [
        "num_folds = 5\n",
        "fold_no = 1\n",
        "loss_per_fold = []\n",
        "minimal_val_losses = []\n",
        "minimal_val_losses_index = []\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "for train, test in kfold.split(table_of_dataX_learn_normed, table_of_dataY_learn):\n",
        "  model = Sequential()\n",
        "  model.add(layers.Dense(75, input_shape=(280,), activation='relu'))\n",
        "  #model.add(layers.Dense(25,activation='relu',))\n",
        "  model.add(layers.Dense(4))\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='mse'\n",
        "              )\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  print(len(table_of_dataX_learn_normed[train]))\n",
        "  #callback = keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
        "  myval_callback = MyCustomCallback()\n",
        "  history = model.fit(table_of_dataX_learn_normed[train], \n",
        "                    table_of_dataY_learn[train], \n",
        "                    epochs=75,\n",
        "                    verbose=1,\n",
        "                    batch_size = 32,\n",
        "                    callbacks = [myval_callback],\n",
        "                    validation_data = (table_of_dataX_learn_normed[test], table_of_dataY_learn[test])\n",
        "                   )\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Minimal loss epoch and value')\n",
        "  min_loss = np.min(history.history['val_loss'])\n",
        "  minimal_val_losses.append(min_loss)\n",
        "  minimal_val_losses_index.append(np.argmin(history.history['val_loss']))\n",
        "  print(min_loss) \n",
        "  scores = model.evaluate(table_of_dataX_learn_normed[test], table_of_dataY_learn[test], verbose=0)\n",
        "  loss_per_fold.append(scores)\n",
        "  fold_no = fold_no + 1\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(loss_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]}')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjDe0faZP8W-"
      },
      "source": [
        "print(minimal_val_losses_index)\n",
        "print(minimal_val_losses)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}